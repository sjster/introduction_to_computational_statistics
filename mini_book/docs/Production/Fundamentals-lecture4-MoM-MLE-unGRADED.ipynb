{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Parameter Estimation\n",
        "\n",
        "Given a representative sample of data from some population, we may need to estimate the parameters for a distribution characterizing the population.  We discus properties of estimators and the following estimation methods below:\n",
        "\n",
        "1. Method of Moments.\n",
        "2. Maximum Likelihood Estimation (MLE).\n",
        "3. Maximum a posteriori probability estimate (MAP).\n",
        "\n",
        "General references:\n",
        "  \n",
        "+ Pattern Recognition and Machine Learning (9780387310732) Bishop, Christopher M.\n",
        "+ Statistical Inference (9780534243128): Casella, George, Berger, Roger L.  \n",
        "+ Probability Theory and Statistical Inference: Empirical Modeling with Observational Data (9781107185142): Spanos, A.  \n",
        "+ Bayesian Models: A Statistical Primer for Ecologists (9780691159287): Hobbs, N. Thompson, Hooten, Mevin B.  \n",
        "+ A First Course in Bayesian Statistical Methods (0387922997): Hoff, Peter D.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<hr style=\"border:2px solid blue\"> </hr>"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "0d37aeb3-2327-4a0d-bc95-5edd3046e858"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Properties of estimators\n",
        "\n",
        "What makes a good estimator?  We likely want an estimator that points to the parameter we are estimating and does not vary much around that value.  Usually, there is a tradeoff between these two desires. A few definitions:\n",
        "\n",
        "#### Consistency\n",
        "\n",
        "Consistency is \n",
        "\n",
        "$$P(|\\theta_n-\\theta |>0) \\to 0 \\text{ as } n \\to \\infty$$\n",
        "\n",
        "In words, this is stating that as the sample gets large, the estimator converges in probability to \\\\(\\theta\\\\).  \n",
        "\n",
        "#### Bias\n",
        "\n",
        "\\\\(\\theta_n\\\\) is unbiased if\n",
        "\n",
        "$$E(\\theta_n)=\\theta$$\n",
        "\n",
        "basically, the estimator is unbiased if it is centered on the true.\n",
        "\n",
        "### Efficiency \n",
        "\n",
        "An estimator that has the lowest possible variance among all unbiased estimators is considered efficient.\n",
        "\n",
        "#### Mean squared error\n",
        "\n",
        "Combining the variance and bias, we get a measure of the quality of the estimator called mean squared error (MSE):\n",
        "\n",
        "$$MSE = variance + bias^2$$\n",
        "\n",
        "MSE is a measure of the trade off between accuracy (spread) and precision (location).\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<hr style=\"border:2px solid blue\"> </hr>"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "aefad9f1-95c4-417e-a655-eee29d2c4505"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Method of moments\n",
        "\n",
        "The method of moments amounts to matching population moments to sample moments.  Basically, we are using the finite approximation given by:  \n",
        "\n",
        "$$E[f] = \\int f(x)^r p(x) dx \\approx \\frac{1}{N} \\sum f(x)^r p(x)$$ \n",
        "\n",
        "where \\\\(f(x)=x\\\\) and \\\\(r=1\\\\), this amounts to \n",
        "\n",
        "$$\\mu \\approx \\bar{x}$$\n",
        "\n",
        "It is up to us to choose which moment to use, however, the number of moments required will be equal to the number of parameters we are looking to estimate.  As an example, suppose we are looking to estimate the probability of success for a coin toss experiment given by the Bernoulli distribution.  We know:\n",
        "\n",
        "\\\\(X_i \\sim Bern(\\theta)\\\\), where we have coded x = 1 for heads:\n",
        "\n",
        "$$P_X(x;\\theta) = \n",
        "\\begin{cases}\n",
        "    \\theta, \\text{for x = 1} \\\\\n",
        "    1 - \\theta, \\text{ for x = 0} \\\\\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Alternatively, we can write this as:\n",
        "\n",
        "$$f(x;\\theta) = \\theta^x(1-\\theta)^{(1-x)}$$\n",
        "\n",
        "For this experiment, let us assume we are collecting N=20 tosses of the dime and end up with data: {1,1,0,1,1,1,1,0,1,0,1,0,1,1,0,0,1,1,1,0} (13 heads).\n",
        "\n",
        "We are looking to estimate both the mean and variance.\n",
        "\n",
        "For the population:\n",
        "\n",
        "$$E[X] = \\sum_x x f(x) = \\sum_x x [\\theta^x(1-\\theta)^{(1-x)}] = 0\\ast(1-\\theta) + 1\\ast\\theta = \\theta$$\n",
        "\n",
        "$$Var(X) = \\theta(1-\\theta)$$ \n",
        "\n",
        "From this, we see we only need to estimate one parameter as the variance is a function of the mean.\n",
        "\n",
        "So, we can match the first sample moment to the population moment to get our estimate:\n",
        "\n",
        "$$\\hat{\\theta} = \\frac{1}{N} \\sum_N x_i = \\frac{13}{20}$$\n",
        "\n",
        "from which the variance can also be computed.\n",
        "\n",
        "Method of Moment estimators can be shown to be consistent, but not necessarily efficient and can give estimates that are outside the parameter space.\n",
        "\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<hr style=\"border:2px solid blue\"> </hr>"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "02856ebe-0252-4c62-a0e1-777f4a23ae17"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Maximum Likelihood Estimation\n",
        "\n",
        "Another approach to parameter estimation follows from an assumption that our data results from  independent and identically distributed observations from a population.  Our goal is to find a $\\theta$ that maximizes the likelihood of us observing our data.  \n",
        "\n",
        "The likelihood function is defined as the joint probability of observing the data:\n",
        "\n",
        "$$\\mathcal{L}(\\theta|x_1 ... x_n) = \\prod_{i=1}^n f(x_i | \\theta)$$\n",
        "\n",
        "Our job is then to solve:\n",
        "\n",
        "\\\\(\\frac{d}{d\\theta}\\mathcal{L}(\\theta|{x})=0\\\\) make sure it is a max and not on the boundary.\n",
        "\n",
        "If we return to the coin toss example with 13 heads in 20 tosses, we start by setting up the likelihood function and differentiating wrt \\\\(\\theta\\\\).\n",
        "\n",
        "\\\\(\\mathcal{L}(\\theta|x_1 ... x_n) = \\prod_{i=1}^n \\theta^x(1-\\theta)^{(1-x)}\\\\)\n",
        "\n",
        "Note, it is often necessary to convert the likelihood to log likelihood to  avoid computational difficulties arising from having lots of data.\n",
        "\n",
        "\\\\(ln \\mathcal{L}(\\theta|x_1 ... x_n) = (\\sum_{i=1}^n x_i) ln \\theta + (\\sum_{i=1}^n (1-x_i)) ln (1-\\theta)\\\\)\n",
        "\n",
        "differentiating wrt to \\\\(\\theta\\\\) and setting to zero, we get\n",
        "\n",
        "\\\\((\\sum_{i=1}^n x_i)\\frac{1}{\\hat{\\theta}} -(\\sum_{i=1}^n (1-x_i))\\frac{1}{1-\\hat{\\theta}}\\\\) = 0\n",
        "\n",
        "solving for \\\\(\\hat{\\theta}\\\\), we end up with \n",
        "\n",
        "$$\\hat{\\theta} = \\frac{1}{n}\\sum_{i=1}^n x_i = \\bar{x}$$\n",
        "\n",
        "Which matches the previous result of \\\\(\\frac{13}{20}\\\\).\n",
        "\n",
        "MLE can be shown to be a consistent estimator, but may be biased.  Operationally, it can be computationally expensive to calculate, but offers a useful fact that any function of the parameters is also a function of the MLE, ie invariant to transformations.\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<hr style=\"border:2px solid blue\"> </hr>"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "cc3e8d40-cf9e-4139-b77b-90fe84419794"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Maximum a posteriori estimate\n",
        "\n",
        "We will discuss MAP estimates in more detail when talking through priors, for now, we can leave this as the MAP estimate is an augmented MLE using prior, or additional information.  The procedure is the same as in finding the MLE, however, we add additional information via:\n",
        "    \n",
        "$$\\hat{\\theta}_{MAP} = arg max_{\\theta} \\mathcal{L}(\\theta|x_1 ... x_n) \\ast \\pi (\\theta)$$\n",
        "\n",
        "\n",
        "\\\\(\\pi (\\theta)\\\\) is our prior or additional information. \n",
        "\n",
        "\n",
        "<br>\n",
        "<br>\n",
        "<hr style=\"border:2px solid blue\"> </hr>"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "fba15e85-8f8a-4076-b6b9-8c479af44e49"
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GRADED EVALUATION (15 mins)\n",
        "\n",
        "1. For a variable \\\\(X_i \\overset{iid} \\sim N(\\mu,\\sigma^2), i=1\\dots n\\\\), find the MLE for \\\\((\\mu ,\\sigma^2)\\\\)   \n",
        "\n",
        "\n",
        "\\\\(\\text{     }(\\hat{ \\mu },\\hat{ \\sigma^2 })\\\\)=   \n",
        "\n",
        "\n",
        "   a. \\\\(\\overline{X}, \\frac{\\sum(X_i-\\overline{X})^2}{n} \\\\)    \n",
        "\n",
        "   b. \\\\(\\overline{X}, \\frac{\\sum(X_i-\\overline{X})^2}{n-1} \\\\)\n",
        "\n",
        "$$$$  \n",
        "2. Does the MLE of the last question agree with the MoM estimator?\n",
        "\n",
        "  a. Yes   \n",
        "  \n",
        "  b. No\n",
        "\n",
        "$$$$\n",
        "3. The MLE estimator is invariant to transformation.  If you want an estimate for \\\\(\\mu^2\\\\) and have the estimate for \\\\(\\mu\\\\), you can simply take the estimate \\\\(\\hat{\\mu}^2\\\\) .\n",
        "\n",
        "  a. False  \n",
        "  \n",
        "  b. True"
      ],
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "fab764b0-1971-4f62-bb30-e91dc83f1869"
        }
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "outputs": [],
      "execution_count": 0,
      "metadata": {
        "application/vnd.databricks.v1+cell": {
          "title": "",
          "showTitle": false,
          "inputWidgets": {},
          "nuid": "01b2ba0e-7859-40f3-8f2b-c47571f29549"
        }
      }
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "mimetype": "text/x-python",
      "name": "python",
      "pygments_lexer": "ipython3",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "version": "3.8.8",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "application/vnd.databricks.v1+notebook": {
      "notebookName": "Fundamentals-lecture4-MoM-MLE-unGRADED",
      "dashboards": [],
      "notebookMetadata": {
        "pythonIndentUnit": 2
      },
      "language": "python",
      "widgets": {},
      "notebookOrigID": 2372252794354814
    },
    "nteract": {
      "version": "0.28.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
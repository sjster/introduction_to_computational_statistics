{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference and Decisions\n",
    "\n",
    "We have been talking about probability and have some tools under our belt for manipulating distributions and estimating parameters.  How do we turn this into a decision?  Here we will discuss some measures that might help inform that decision.\n",
    "\n",
    "  \n",
    "General references:\n",
    "  \n",
    "+ Statistical Inference (9780534243128): Casella, George, Berger, Roger L.  \n",
    "+ Probability Theory and Statistical Inference: Empirical Modeling with Observational Data (9781107185142): Spanos, A.  \n",
    "+ Bayesian Models: A Statistical Primer for Ecologists (9780691159287): Hobbs, N. Thompson, Hooten, Mevin B.  \n",
    "+ A First Course in Bayesian Statistical Methods (0387922997): Hoff, Peter D.\n",
    "  \n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<hr style=\"border:2px solid blue\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classification\n",
    "\n",
    "In the last section, we gave an example taking of a test for determining status of disease.  We took the test and discovered a positive result which led to a probability that we are truly infected given the postivite test of 79%.  The odds are, we have the disease.  Are we sure enough that we will seek appropriate treatment or perhaps we want a second opinion?  Is there a second test?  Do we simply take two tests and combine the results?  When we build a computational model, we repeatedly ask this question and have to make a decision, do we continue in the current direction or go from whence we came. The consequences of making a wrong choice are often not symmetric.  Our overall goal is to simply be right, but understanding consequence discrepancies may suggest wrong answers are not all the same.  In this example we may choose to weigh a wrong answer differently in one direction than another.\n",
    "\n",
    "Restating this in terms of probability, for a binary decision, our goal is:\n",
    "\n",
    "\\\\(\\text{argmin } p(mistake) = \\sum_{i=1}^k p(x_{k \\notin j},C_k)\\\\) where j/k are class labels.\n",
    "\n",
    "#### Rejection regions or decision boundaries\n",
    "\n",
    "In our decision, we may choose a region for inclusion in one class or another, for instance, we may decide the optimal boundary in our disease classification is at $p(disease|+test)>75%$.  Note that the decision boundary should parition the space.  Written with a decision boundary, \n",
    "\n",
    "$$\\text{argmin } p(mistake) = \\int_{\\mathbb{R_1}} p(x,C_2) dx + \\int_{\\mathbb{R_2}} p(x,C_1) dx$$\n",
    "\n",
    "\n",
    "Returning to the coin toss experiment, after we perform the experiment many times, we can arrange the data and find the regions to the right/left that give the desired partions between 'fair' and 'biased' based on our tolerance for error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bounds are:\n",
      "[0.35, 0.6]\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random as random\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# define an experiment as flipping Ncoins\n",
    "# we will repeat the experiment Ntrials times\n",
    "# for each experiment, we record the fraction of heads(1's) observed\n",
    "\n",
    "def do_one_trial(pheads=0.5, Ncoins=10):\n",
    "    coin = [1,0] #H=1, T=0\n",
    "    total_flips = 1000 \n",
    "    toss_results = random.choices(coin,k=Ncoins,weights=[pheads,1-pheads])\n",
    "    fraction_successes = float(sum(toss_results)/Ncoins) # success = head\n",
    "    return fraction_successes\n",
    "\n",
    "def do_many_trials(pheads=0.5, Ntrials=1000, Ncoins=20):\n",
    "    resultarr = []\n",
    "    for i in range(0, Ntrials):\n",
    "        resultarr.append(do_one_trial(pheads=pheads, Ncoins=Ncoins))\n",
    "    return resultarr\n",
    "\n",
    "# looking to get the bounds of the rejection region for plotting\n",
    "def get_thresh_2tail(trial_data, significance=.1):\n",
    "    sorted_results = np.array(sorted(trial_data))\n",
    "    twotailedsig = significance/2.\n",
    "    # sum from left to right to find left most bound\n",
    "    for val in sorted_results:\n",
    "        integral = float(len(sorted_results[sorted_results<val]))/len(sorted_results)\n",
    "        if integral > twotailedsig:\n",
    "            lowerbound = val\n",
    "            break\n",
    "    # sum from right to left to find right most bound\n",
    "    for val in sorted_results[::-1]:\n",
    "        integral = float(len(sorted_results[sorted_results>val]))/len(sorted_results)\n",
    "        if integral > twotailedsig:\n",
    "            upperbound = val\n",
    "            break\n",
    "    return lowerbound, upperbound\n",
    "\n",
    "coin_experiment_data = do_many_trials(pheads=0.5, Ntrials=100, Ncoins=20)\n",
    "lowerbound, upperbound = get_thresh_2tail(coin_experiment_data, significance=0.1)\n",
    "\n",
    "# plot data with rejection regions\n",
    "hist, bins = np.histogram(coin_experiment_data, bins = np.linspace(0,1,11), density=True)\n",
    "hist = np.concatenate([hist,[0.]])\n",
    "plt.plot(bins, hist, drawstyle='steps-post')\n",
    "plt.xlabel('Fraction of Heads')\n",
    "plt.axvline(lowerbound, color='orange')\n",
    "plt.axvline(upperbound, color='orange')\n",
    "plt.xlim(0, 1)\n",
    "\n",
    "print(\"bounds are:\")\n",
    "print([lowerbound, upperbound])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing we are going to make mistakes, we introduce the idea of a loss function.  Loss functions (also called cost functions or as a positive, a utility functions), is a function that we create that captures the goal of minimizing errors but allows for differences in magnetude for consequences of the mistakes.  The goal is now to minimize the average loss:\n",
    "\n",
    "$$E[loss] = \\sum_k \\sum_j \\int_{\\mathbb{R_j}} L_{kj}p(x,C_k) dx$$\n",
    "\n",
    "\n",
    "#### Loss functions\n",
    "\n",
    "Loss functions are functions created to match our goals with penalties to steer our algorithm to beneficial regions.  As an example, consider the follow loss function:\n",
    "\n",
    "$$\\begin{matrix} & \\text{disease} & \\text{no disease} \\\\ \\text{disease} & 0 & 100 \\\\\\text{no disease} & 1 & 0 \\end{matrix}$$\n",
    "\n",
    "Here we are assigning zero loss for correct responses, 1 loss for cases where we predict \"no disease\" but the true state is \"disease\" and finally, 100 loss for cases of incorrectly predicting \"disease\".  The loss function here is implicitly penalizing false positives more than false negatives (see Confusion Matrix at end).\n",
    "\n",
    "$$L(\\theta,a) = \n",
    "\\begin{cases}\n",
    "    0, \\text{for {(predict disease, actual disease),(predict no disease, actual no disease)}} \\\\\n",
    "    1, \\text{for predict no disease, actual disease} \\\\\n",
    "    100, \\text{for predict disease, actual no disease} \\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "\n",
    "#### Cross Entropy Loss\n",
    "\n",
    "Any loss function which is the negative log likelihood between the empirical distribution of the data and the probability distribution of the model is a cross entropy loss.\n",
    "\n",
    "Often, when one talks about cross entropy loss, the meaning is for binary classification.  In fact, cross entropy loss, also called log loss, is the go-to function for binary classification tasks.  It is the log likelihood for a Bernoulli trial.\n",
    "\n",
    "$$-y_i ln(\\hat{\\theta}_i) - (1-y_i) ln (1-\\hat{\\theta}_i)$$\n",
    "\n",
    "Cross entropy loss is nice in that it penalizes heavily for very wrong results.  In this case, the true labels are the \\\\(y_i\\\\)'s while our predictions are the associated probabilities (\\\\(\\hat{\\theta}_i\\\\)'s). Note, there is no 'reward' in cross entropy loss, the best you can do is 0 when the output is correct.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Penalties for each prediction\n",
      "[[-0.05129319 -0.        ]\n",
      " [-1.89711932 -0.        ]\n",
      " [-0.         -1.38629396]\n",
      " [-0.         -0.04082189]]\n",
      "Total cross entropy loss is: 0.8438820897043499\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "predictions = np.array([[0.95,0.05],\n",
    "                        [0.15,0.85],\n",
    "                        [0.75,0.25],\n",
    "                        [0.04,0.96]]) #predictions are A,B,B,A\n",
    "targets = np.array([[1,0],\n",
    "                    [1,0],\n",
    "                    [0,1],\n",
    "                   [0,1]]) #correct classes are A,A,B,B\n",
    "\n",
    "epsilon=1e-10 # avoid log of zero\n",
    "predictions = np.clip(predictions, epsilon, 1. - epsilon)\n",
    "N = predictions.shape[0]\n",
    "x = targets * np.log(predictions+1e-7)\n",
    "print(\"Penalties for each prediction\")\n",
    "print(x)\n",
    "ce_loss = -np.sum(x)/N\n",
    "print (\"Total cross entropy loss is: \" + str(ce_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that this can easily be extended to the case of multiple categories.\n",
    "\n",
    "#### Categorical cross entropy loss\n",
    "\n",
    "$$\\text{CE loss }= - \\sum_{i=1}^I \\sum_{k=1}^K \\mathbb{1}_{y_i = k}log p(y_i = k | x_i,\\theta)$$\n",
    "\n",
    "This looks completely different, but understanding the terms will quickly lead to a familiar setup.  The CE loss is a sum of the probabilities assigned to the predicted class.  The \\\\(\\mathbb{1}\\\\) is an idicator function determining which class is the correct class while the \\\\(p(y_i = k | x_i,\\theta)\\\\) is the likelihood of the correct class given the data.  In the case of two categories, this reduces to the cross entropy case above.\n",
    "\n",
    "\n",
    "#### Other classification loss functions\n",
    "\n",
    "**Focal loss** is a modified loss attempting to highlight the loss from incorrectly classified samples vs those that are classified correctly, but perhaps with less than perfect probability.\n",
    "\n",
    "$$FL = - \\sum_{i=1}^{C=2}(1-\\hat{\\theta}_i)^\\gamma t_i log(\\hat{\\theta}_i)$, where $(1-\\hat{\\theta}_i)^\\gamma$$\n",
    "\n",
    "modulates the loss.  As the probability of the assignment goes towards 1 for correct responses, the contribution to the loss goes to 0 more quickly.\n",
    "\n",
    "**Hinge loss** is a maximum margin classifier.  Basically, this loss aims to make the classification more of a sure event.  The score of a correct assignment should exceed the sum of scores of incorrect assignments by some margin.\n",
    "\n",
    "$$HL = \\sum_{j \\ne y_i} max(0,s_j-s_{y_i}+1)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Continuous response\n",
    "\n",
    "In continuous responses, the goodness of fit measure is some function of distance.  \n",
    "\n",
    "##### Mean squared error \n",
    "\n",
    "Mean squared error, MSE, is defined as \n",
    "\n",
    "$$MSE = \\frac{1}{N} \\sum_{i=1}^N (y_i - \\hat{y_i})^2$$\n",
    "\n",
    "This has a number of nice features as a loss function including penalizing based on distance from the prediction and mathmatical properties such as that it is differentiable and there is always a single solution.  MSE is sensitive to outliers, ie it is not robust to outliers.\n",
    "\n",
    "##### Mean absolute error \n",
    "\n",
    "Mean absolute error, MAE, is defined as \n",
    "\n",
    "$$MAE = \\frac{1}{N} \\sum_{i=1}^N |y_i - \\hat{y_i}|$$\n",
    "\n",
    "Like MSE, MAE returns the average of the magnetude of errors, but needs computational tools to deal with the lack of a differential.  MAE is less sensitive to ouliers than MSE, however, there is no guarantee of a unique solution.\n",
    "\n",
    "##### L1 and L2 loss\n",
    "\n",
    "L1 loss is sum of absolute differences, also called the method of Least Absolute Deviations (LAD).  \\\\(L_1 = N \\ast MAE\\\\).  \n",
    "\n",
    "L2 loss is \\\\(N \\ast MSE\\\\) also called least squares error, or LSE.\n",
    "\n",
    "##### Regularization \n",
    "\n",
    "Regularization is a technique to prevent overfitting throug addition of penatly term to the sum of the regression coefficents.  For instance:\n",
    "\n",
    "\\\\(argmin LSE + \\lambda \\sum_{i=1}^k|w_i|\\\\), where the \\\\(w_i\\\\) are the weights being learned.\n",
    "\n",
    "The above woulld be an L1 regularization on least squares (LASSO).  There are other variations including L2 regularization (ridge regression) where the penalty is on the squared weights, elastic net (both L1 and L2 regularization), etc.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Confusion matrix\n",
    "\n",
    "For binary classification problems, we have the so called confusion matrix.  For a class known to be A, did we predict A or B?  The result to that question can be summarized in the confusion matrix:\n",
    "\n",
    "\n",
    "|                     | Positive Class                    | Negative Class                       |\n",
    "|---------------------|-----------------------------------|--------------------------------------|\n",
    "| Positive Prediction | True Positive (TP)                | False Positive (FP) **Type I error** |\n",
    "| Negative Prediction | False Negative (FN) **Type II error** | True Negative (TN)              |\n",
    "\n",
    "The various cells give rise to the 4 possible cases: TP,FP,FN,TN.  Associated with these cases are marginal rates:\n",
    "\n",
    "True Positive Rate (TPR): TP/(TP+FN)  -- **Sensitivity**  \n",
    "False Positive Rate (FPR): FP/(FP+TN)  \n",
    "True Negative Rate (TNR): TN/(TN+FP)  -- **Specificity**  \n",
    "False Negative Rate (FNR): FN/(FN+TP)\n",
    "\n",
    "As an aside, we can map these to Bayes' Rule where\n",
    "\n",
    "P(A) = Probability of Positive Class (PC)  \n",
    "P(not A) = Probability of Negative Class (NC)  \n",
    "P(B) = Probability of Positive Prediction (PP)  \n",
    "P(not B) = Probability of Negative Prediction (NP)  \n",
    "\n",
    "$$P(A|B) = \\frac{P(B|A)*P(A)}{P(B)} = \\frac{TPR*PC}{TPR*PC + FPR*NC}$$\n",
    "\n",
    "\n",
    "Precision also called the Positive Predictive Value.  \n",
    "$$precision = PPV = \\frac{TP}{TP+FP} = \\frac{TPR*PC}{PP}$$\n",
    "\n",
    "Recall is also known as the sensitivity.  \n",
    "$$recall = \\frac{TP}{TP+FN}$$\n",
    "\n",
    "$$F_1 score = \\frac{precision*recall}{precision+recall}$$\n",
    "\n",
    "Type 1 error = false positive\n",
    "\n",
    "Type 2 error = false negative\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "<hr style=\"border:2px solid blue\"> </hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### UNGRADED EVALUATION (20 min)\n",
    "\n",
    "#### 1. Categorical cross entropy can be used when multiple classes are to be differentiated.  For each sample, you expect the output of the algorithm to give\n",
    "\n",
    "    a. A single probability giving the probability of being correct  \n",
    "    \n",
    "    b. Multiple probabilities representing the probability of assignment to each class    \n",
    "    \n",
    "#### 2. Mean squared error is an example of\n",
    "\n",
    "    a. A continuous loss function penalizing wrong answers by the squared distance between true and predicted repsonse.     \n",
    "    \n",
    "    b. A mathmatical construct representing the mean number of responses incorrectly predicted in a classification problem.\n",
    "    \n",
    "#### 3. Cross entropy loss gives a penalty to incorrectly classified samples and rewards correct classification.\n",
    "\n",
    "    a. false    \n",
    "    b. true\n",
    "\n",
    "\n",
    "#### 3. You are building a classifier to classify dogs vs cats.  Where dogs =1, your algorithm is presented with the following set of images {1 0 1 0 1 0 0 1 1 1} and gives the following predictions {1 0 0 0 1 1 1 0 0 1}.  With respect to classifying dogs, the sensitivity is:\n",
    "\n",
    "    a. 0.5     \n",
    "    \n",
    "    b. 0.83 \n",
    "    \n",
    "    c. 6\n",
    "\n",
    "#### 4. In the dog classification problem, correctly classifying cats as cats will give the\n",
    "\n",
    "    a. true positives \n",
    "    \n",
    "    b. false positives  \n",
    "    \n",
    "    c. false negatives  \n",
    "    \n",
    "    d. true negatives     \n",
    "\n",
    "#### 5.  You are building a spam classifier, \n",
    "\n",
    "    a. false positives are more problematic than false negatives  \n",
    "    \n",
    "    b. false negatives are more problematic than false positives  \n",
    "    \n",
    "    c both false positives and false negatives are equally problematic     \n",
    "\n",
    "#### 6. In your spam classifier, you choose to weight the choice as spam or not differently, you should  \n",
    "\n",
    "    a. choose a penalized regression to dampen the weights \n",
    "    \n",
    "    b. choose a custom categorical loss function to allow setting importance of decisions      \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GRADED EVALUATION (15 mins)\n",
    "\n",
    "1. Cost functions are measures of fit.  \n",
    "\n",
    "    a. true     \n",
    "    \n",
    "    b. false \n",
    "\n",
    "2. Cost functions are only useful in categorical decision settings.\n",
    "\n",
    "    a. false      \n",
    "    \n",
    "    b. true\n",
    "\n",
    "3. The following is a valid example of a cost function:\n",
    "\n",
    "    a. \n",
    "\n",
    "    $$L(\\theta,a) = \n",
    "    \\begin{cases}\n",
    "        0, \\text{for {(predict disease, actual disease),(predict no disease, actual no disease)}} \\\\\n",
    "        1, \\text{for predict no disease, actual no disease} \\\\\n",
    "        100, \\text{for predict disease, actual no disease} \\\\\n",
    "    \\end{cases}\n",
    "    $$\n",
    "\n",
    "    b.   \n",
    "\n",
    "    $$L(\\theta,a) = \n",
    "    \\begin{cases}\n",
    "        0, \\text{for {(predict disease, actual disease),(predict no disease, actual no disease)}} \\\\\n",
    "        1, \\text{for predict no disease, actual disease} \\\\\n",
    "        100, \\text{for predict disease, actual no disease} \\\\\n",
    "    \\end{cases}\n",
    "    $$\n",
    "\n",
    "2. In a recent study, you created a cost function for classification of 4 classes.  Following training, you obtained the following resuls.  What are the predictions for the samples in terms of thier class?\n",
    "\n",
    "    predictions = np.array([[0.25,0.25,0.20,0.30],\n",
    "                            [0.25,0.30,0.20,0.25],\n",
    "                            [0.10,0.25,0.25,0.40],\n",
    "                            [0.45,0.10,0.20,0.25],\n",
    "                            [0.01,0.01,0.01,0.96]])\n",
    "\n",
    "    a. A,B,C,B\n",
    "    \n",
    "    b. D,B,D,A,D   \n",
    "    \n",
    "    c. D,B,C,E\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

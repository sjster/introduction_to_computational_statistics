
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Topics in Model Performance &#8212; Introduction to Computational Statistics with PyMC3</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.acff12b8f9c144ce68a297486a2fa670.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
      <img src="../_static/logo_large.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Computational Statistics with PyMC3</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  Getting started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Production/About.html">
   The What, Why and Whom…
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/getting_started.html">
   Setting up Your Python Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Databricks.html">
   Introduction to the Databricks Environment
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction to Bayesian Statistics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Fundamentals-lecture1-belief-and-probability-unGRADED.html">
   Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Fundamentals-lecture2-manipulating-probability-unGRADED.html">
   Probability - II
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Fundamentals-lecture3-intro-distributions-unGRADED.html">
   Distributions, central tendency, and shape parameters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Fundamentals-lecture4-MoM-MLE-unGRADED.html">
   Parameter Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Fundamentals-lecture5a-Basics-of-Bayes-unGRADED.html">
   Introduction to the Bayes Theorem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Fundamentals-lecture5b-inference-decisions-unGRADED.html">
   Inference and Decisions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Fundamentals-lecture6-priors-unGRADED.html">
   Priors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Intro.html">
   Bayesian vs. Frequentist Statistics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Distributions.html">
   Introduction to Common Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Sampling.html">
   Sampling Algorithms
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction to Monte Carlo Methods
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Production/BayesianInference.html">
   Topics in Model Performance
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/MonteCarlo.html">
   Introduction to Monte Carlo Methods
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  PyMC3 for Bayesian Modeling and Inference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Production/PyMC3.html">
   Introduction to PyMC3
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Reparameterization.html">
   Centered vs. Non-centered Parameterization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Covid_modeling_problem.html">
   Covid Modeling with PyMC3 - Problem Statement
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Covid_modeling.html">
   Covid Modeling with PyMC3
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/docs/BayesianInference.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sjster/statistical_computing_book/master?urlpath=tree/mini_book/docs/BayesianInference.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/sjster/statistical_computing_book/blob/master/mini_book/docs/BayesianInference.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#underfitting-vs-overfitting">
   Underfitting vs. Overfitting
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#r-2-and-explained-variance">
   <span class="math notranslate nohighlight">
    \(R^2\)
   </span>
   and Explained Variance
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#what-does-it-do">
     What does it do?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#derivation">
     Derivation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#how-do-we-interpret-this">
     How do we interpret this?
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#explained-variance">
     Explained Variance
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#measures-for-predictive-performance">
   Measures for Predictive Performance
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#cross-validation">
     1. Cross-Validation
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#information-criteria">
     2. Information criteria
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#log-likelihood-and-deviance">
     Log-likelihood and Deviance
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#log-likelihood-log-predictive-density">
       Log-likelihood (Log predictive density)
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#deviance">
       Deviance
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#why-use-the-deviance-over-the-log-likelihood">
       Why use the Deviance over the Log-likelihood?
      </a>
     </li>
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#a-note-on-mle">
       A Note on MLE
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#posterior-predictive-distribution-to-estimate-predictive-accuracy">
     Posterior Predictive Distribution to Estimate Predictive Accuracy
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#akaike-information-criterion-aic">
     Akaike Information Criterion (AIC)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#bayesian-information-criterion-bic">
     Bayesian Information Criterion (BIC)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#deviance-information-criterion-dic">
     Deviance Information Criterion (DIC)
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#widely-applicable-information-criterion-waic">
     Widely Applicable Information Criterion (WAIC)
    </a>
    <ul class="nav section-nav flex-column">
     <li class="toc-h4 nav-item toc-entry">
      <a class="reference internal nav-link" href="#log-pointwise-predictive-density-lppd">
       Log pointwise predictive density (lppd)
      </a>
     </li>
    </ul>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#a-qualitative-discussion">
     A Qualitative Discussion
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#entropy-and-kl-divergence">
   Entropy and KL Divergence
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#entropy">
     Entropy
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ungraded-evaluation-15-mins">
   UNGRADED EVALUATION (15 mins)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#kl-divergence">
     KL Divergence
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ungraded-evaluation-1-hr">
   UNGRADED EVALUATION (1 hr)
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#model-averaging">
   Model Averaging
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#pseudo-bayesian-modeling-averaging">
     Pseudo Bayesian Modeling Averaging
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#stacking">
     Stacking
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graded-evaluation-30-mins">
   GRADED EVALUATION (30 mins)
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="topics-in-model-performance">
<h1>Topics in Model Performance<a class="headerlink" href="#topics-in-model-performance" title="Permalink to this headline">¶</a></h1>
<p><strong>References</strong></p>
<ul class="simple">
<li><p>Bayesian Analysis with Python: Introduction to Statistical Modeling and Probabilistic Programming with PyMC3 and ArViz, 2nd Edition</p></li>
</ul>
<p>You may have heard the saying</p>
<p><code class="docutils literal notranslate"><span class="pre">All</span> <span class="pre">models</span> <span class="pre">are</span> <span class="pre">wrong,</span> <span class="pre">but</span> <span class="pre">some</span> <span class="pre">are</span> <span class="pre">useful</span></code></p>
<p>This aphorism is attributed to the British statistician George E.P. Box. The implication of this is that models are supposed to represent reality but they rarely do in full fidelity. However, some models can provide useful insight while others add little value to what we already know, or worse provide us misleading information.</p>
<p>So how do we determine if our model can be trusted? In Machine Learning, we resort to Cross-Validation to provide us with a sense of certainty about a model’s ability to generalize to unseen data. Let us look at some other measures with roots in information theory to assess the predictive ability of our models.</p>
<div class="section" id="underfitting-vs-overfitting">
<h2>Underfitting vs. Overfitting<a class="headerlink" href="#underfitting-vs-overfitting" title="Permalink to this headline">¶</a></h2>
<p>Most folks have by now heard of underfitting and overfitting a model. Simpler models should be preferred but not at the cost of accuracy. An overfit model, on the other hand, may not generalize well on new data. We can measure how well a model fits the data using the <span class="math notranslate nohighlight">\(R^2\)</span> metric which measures the proportion of explained variance.</p>
<p>If we use the example of linear regression and start with a first order polynomial regression to explain the data, we may find that the data may not be adequately captured. This is referred to as underfitting. We may have to incrementally increase the complexity of the model by increasing the order of the polynomial. Past a certain point, however, the model starts overfitting to the data. What this means is that the model simply used its representational power to memorize the data and will perform poorly on new data that is fed into the model (\(R^2\) too high).</p>
<p>We want a model that has found that balance between being underfit and overfit, this trade-off is often referred to as the bias-variance trade-off. Bias is the error in the data resulting from its inability to accomodate the data. The model does not have the representational power to capture all the variations and patterns in the data. Variance is the error resulting from the sensitivity of the model to the data, which usually results from too complex of a model. Regularization is often used for this reason to reduce the complexity in a regression model (or neural network) by minimizing the number of coefficients.</p>
<p><img alt="Underfitting vs. Overfitting (from AWS)" src="https://docs.aws.amazon.com/machine-learning/latest/dg/images/mlconcepts_image5.png" /></p>
<center>Underfitting vs Overfitting (from AWS  docs)</center>
</div>
<div class="section" id="r-2-and-explained-variance">
<h2><span class="math notranslate nohighlight">\(R^2\)</span> and Explained Variance<a class="headerlink" href="#r-2-and-explained-variance" title="Permalink to this headline">¶</a></h2>
<div class="section" id="what-does-it-do">
<h3>What does it do?<a class="headerlink" href="#what-does-it-do" title="Permalink to this headline">¶</a></h3>
<p><span class="math notranslate nohighlight">\(R^2\)</span> is a goodness-of-fit measure that tells you how well the data fits the model that we created. More pedantically, it explains the proportion of variance in the outcomes that the independent variables explain.</p>
</div>
<div class="section" id="derivation">
<h3>Derivation<a class="headerlink" href="#derivation" title="Permalink to this headline">¶</a></h3>
<p>If we observe data given by <span class="math notranslate nohighlight">\(y_i\)</span> such that the fitted model predicts <span class="math notranslate nohighlight">\(f_i\)</span> for each point i, we can write the mean of all the observed data, given by \(y_{mean}\) as</p>
<div class="math notranslate nohighlight">
\[y_{mean} = \dfrac{1}{n} \sum_i y_i\]</div>
<ul class="simple">
<li><p>Total sum of squares, which is proportional to the variance of the data, is</p></li>
</ul>
<div class="math notranslate nohighlight">
\[SS_{tot} = \sum_i (y_i - y_{mean})^2\]</div>
<ul class="simple">
<li><p>The residual sum of squares (also called the error) is defined as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[SS_{res} = \sum_i (y_i - f_i)^2\]</div>
<ul class="simple">
<li><p>Now, \(R^2\) is defined as</p></li>
</ul>
<div class="math notranslate nohighlight">
\[R^2 = 1 - \dfrac{SS_{res}}{SS_{tot}}\]</div>
</div>
<div class="section" id="how-do-we-interpret-this">
<h3>How do we interpret this?<a class="headerlink" href="#how-do-we-interpret-this" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>\(R^2\) is 1 for a model that perfectly fits the observed data, i.e. \(f_i = y_i\) for all i.</p></li>
<li><p>If the model predicts \(y_{mean}\) always then \(SS_{res} = SS_{tot}\) and \(R^2 = 0\), this indicates a baseline model to which all other models can be compared.</p></li>
<li><p>Any model that performs worse than the baseline model will have a negative <span class="math notranslate nohighlight">\(R^2\)</span> score.</p></li>
</ul>
</div>
<div class="section" id="explained-variance">
<h3>Explained Variance<a class="headerlink" href="#explained-variance" title="Permalink to this headline">¶</a></h3>
<p>The term <span class="math notranslate nohighlight">\(\dfrac{SS_{res}}{SS_{tot}}\)</span> is also called Unexplained Variance. Therefore</p>
<p><span class="math notranslate nohighlight">\(R^2 = 1 - Unexplained Variance\)</span></p>
<p>or</p>
<p><span class="math notranslate nohighlight">\(R^2 = Explained Variance\)</span></p>
</div>
</div>
<div class="section" id="measures-for-predictive-performance">
<h2>Measures for Predictive Performance<a class="headerlink" href="#measures-for-predictive-performance" title="Permalink to this headline">¶</a></h2>
<p>Accuracy of the model can be measured by the following methods:</p>
<div class="section" id="cross-validation">
<h3>1. Cross-Validation<a class="headerlink" href="#cross-validation" title="Permalink to this headline">¶</a></h3>
<p>Here we divide the data into non-overlapping subsets and perform training and validation on the different subsets. Depending on how we perform this Cross-Validation, it can be called K-fold Cross-Validation or leave-one-out Cross-Validation (LOOCV). In <strong>K-fold Cross-Validation</strong> we divide the data into ‘K’ folds or subsets, perform training of the model on k-1 folds while the model performance is assessed on the 1 fold that was left. We iteratively select each fold to be the test fold while the others become the training folds.</p>
<p><img alt="Image from the scikit-learn page for K-fold cross validation" src="https://scikit-learn.org/stable/_images/grid_search_cross_validation.png" /></p>
<center><i>K-fold Cross-Validation from the scikit-learn page</i></center>
<p>If the number of folds is equal to the number of data points, we have <strong>Leave-One-Out Cross-Validation</strong>.</p>
</div>
<div class="section" id="information-criteria">
<h3>2. Information criteria<a class="headerlink" href="#information-criteria" title="Permalink to this headline">¶</a></h3>
<p><strong>Reference</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.casact.org/education/rpm/2016/presentations/PM-LM-4-Tevet.pdf">Predictive metrics presentation from Liberty Mutual</a></p></li>
</ul>
<p>A number of ideas that are firmly rooted in information theory help us to quantify how well a model performs.</p>
<ol class="simple">
<li><p>Log-likelihood (Log predictive density) and deviance</p></li>
<li><p>Akaike Information Criterion (AIC)</p></li>
<li><p>Widely Applicable Information Criterion (WAIC)</p></li>
<li><p>Deviance Information Criterion (DIC)</p></li>
<li><p>Bayesian Information Criterion (BIC)</p></li>
</ol>
<p>For (2) to (5),</p>
<ul>
<li><p>They take the form of the equation with two terms given by</p>
<div class="math notranslate nohighlight">
\[metric = model\;fit + penalization\]</div>
</li>
<li><p>The model fit is measured using the log likelihood of the data given model parameters (could be a pointwise estimate, or could use the full posterior distribution)</p></li>
<li><p>Lower values imply a better fit</p></li>
</ul>
<p>AIC, BIC and DIC use the joint probability of the data, whereas WAIC computes the pointwise probability of the data. In the following, we assume the model parameters are independent, thereby the joint probability is the same as the product of the pointwise estimates.</p>
</div>
<div class="section" id="log-likelihood-and-deviance">
<h3>Log-likelihood and Deviance<a class="headerlink" href="#log-likelihood-and-deviance" title="Permalink to this headline">¶</a></h3>
<p><strong>Reference</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://www.erudit.org/en/journals/mee/2015-v37-n3-mee02497/1036328ar/">Cousineau, Denis et Teresa A. Allan. “Likelihood and its use in Parameter Estimation and Model Comparison.” Mesure et évaluation en éducation, volume 37, number 3, 2015, p. 63–98. https://doi.org/10.7202/1036328ar</a></p></li>
</ul>
<p>These terms are used to measure the error in our model with regards to the data that the model is trying to fit. Most folks are familiar with the Mean Squared Error (MSE) given by</p>
<div class="math notranslate nohighlight">
\[MSE = \sum_1^n (y_{true} - y_{predicted})^2 / n\]</div>
<div class="section" id="log-likelihood-log-predictive-density">
<h4>Log-likelihood (Log predictive density)<a class="headerlink" href="#log-likelihood-log-predictive-density" title="Permalink to this headline">¶</a></h4>
<p>While this is a perfectly acceptably way of measuring error especially if the likelihood is a normal distribution, a more theoretically justified way to measure the performance of a model is using the log-likelihood function.</p>
<div class="math notranslate nohighlight">
\[Loglikelihood = \sum_1^n log p(y_i | \theta)\]</div>
<p>If the likelihood function is a Normal, the log-likelihood is proportional to the MSE.</p>
</div>
<div class="section" id="deviance">
<h4>Deviance<a class="headerlink" href="#deviance" title="Permalink to this headline">¶</a></h4>
<p>Deviance is two times the log-likelihood of the model subtracted from the log-likelihood of a saturated model. A saturated model is one that has overfitted to the point that it fits the observed data perfectly. It can be rewritten to emphasize that the range of values are now from 0 to \(\infty\).</p>
<div class="math notranslate nohighlight">
\[Deviance = -2 \sum_1^n ( log p(y_i | \theta) - log p(y_i | \theta_s) \]</div>
</div>
<div class="section" id="why-use-the-deviance-over-the-log-likelihood">
<h4>Why use the Deviance over the Log-likelihood?<a class="headerlink" href="#why-use-the-deviance-over-the-log-likelihood" title="Permalink to this headline">¶</a></h4>
<p>Note that the likelihood function <span class="math notranslate nohighlight">\(p(y_i | \theta)\)</span> takes values from 0 for no fit to 1 for a perfectly fit model. This results in the log-likelihood function taking values from <span class="math notranslate nohighlight">\(- \infty\)</span> to 0. Multiplying the log-likelihood function by -2 results in a number that is interpretable similar to the MSE.</p>
<ul class="simple">
<li><p>Poorly fit models have large positive values</p></li>
<li><p>A perfectly fit model has a value of 0.</p></li>
</ul>
<p>Complex models will have lower deviance values on the training set (in-sample data), and this needs to be penalized when comparing models. This is related to model overfitting that we talked about earlier.</p>
</div>
<div class="section" id="a-note-on-mle">
<h4>A Note on MLE<a class="headerlink" href="#a-note-on-mle" title="Permalink to this headline">¶</a></h4>
<p>Maximum Likelihood Estimation (MLE) is based on the notion of estimating the parameters <span class="math notranslate nohighlight">\(\theta\)</span> that maximize the the probability <span class="math notranslate nohighlight">\(\sum_1^n p(y_i | \theta)\)</span>. While there are other methods to do the same, with a large enough sample size MLE is the most efficient estimator for the distribution parameter <span class="math notranslate nohighlight">\(\theta\)</span>. Also, as sample size increases the estimated parameter tends to the true parameter and the error becomes normally distributed.</p>
<p>A disadvantage of the MLE arises when you have non-regular distributions, i.e. distributions whose parameters are constrained by the observed values. For such distributions, a maximum likelihood may not exist. Similar problems can occur in cases where multiple maxima exist.</p>
</div>
</div>
<div class="section" id="posterior-predictive-distribution-to-estimate-predictive-accuracy">
<h3>Posterior Predictive Distribution to Estimate Predictive Accuracy<a class="headerlink" href="#posterior-predictive-distribution-to-estimate-predictive-accuracy" title="Permalink to this headline">¶</a></h3>
<p>This also sets the stage to be more Bayesian by utilizing the <strong>posterior predictive distribution</strong>. This allows us to measure the model’s probability of generating the new data i.e. \(p(y_{new} | y)\). This can be interpreted as asking “What is the probability of seeing the new out-of-sample-data, given the model that was trained on the in-sample data?”. The predictive accuracy can be written as</p>
<div class="math notranslate nohighlight">
\[accuracy = p(y_{new} | y) = \int p(y_{new} | \theta) p(\theta | y) d \theta\]</div>
<p>where \(p(\theta | y)\) is the posterior distribution for \(\theta\) and we integrate over the entire distribution of \(\theta\). Now this is simply the expectation of \(p(y_{new} | \theta)\) over the posterior distribution of \(\theta\). In simple terms, it is the average of all the probabilities of seeing \(y_{new}\) calculated over all possible values of \(\theta\).</p>
<div class="math notranslate nohighlight">
\[accuracy = E [ p(y_{new} | \theta) ]\]</div>
<p>This has the following steps</p>
<ol class="simple">
<li><p>Draw a \(\theta_i\) from the posterior distribution for \(\theta\)</p></li>
<li><p>Given the value of \(\theta_i\) how likely are you to see \(y_{new}\), or compute \(p(y_{new} | \theta_i)\)</p></li>
<li><p>Repeat (1) and (2) several times to compute the expectation of \(p(y_{new} | \theta)\)</p></li>
</ol>
<p>This is also computed using the log frequently as</p>
<div class="math notranslate nohighlight">
\[accuracy = log( E [ p(y_{new} | \theta) ])\]</div>
</div>
<div class="section" id="akaike-information-criterion-aic">
<h3>Akaike Information Criterion (AIC)<a class="headerlink" href="#akaike-information-criterion-aic" title="Permalink to this headline">¶</a></h3>
<p>The AIC is derived from the world of Frequentist statistics and does not use the posterior distribution. Therefore, instead of integrating over the posterior, it uses the MLE estimate for \(\theta\). The term \(E [ p(y_{new} | \theta) ]\) is now replaced with \(p(y_{new} | \theta_{mle})\).</p>
<div class="math notranslate nohighlight">
\[AIC = -2 \sum_{i=1}^n log p(y_i | \theta_{mle}) + 2 n_{parameters}\]</div>
<p>Here \(n_{parameters}\) refers to the number of parameters in the model and \(\theta_{mle}\) is the MLE estimate of \(\theta\). We want a model with a lower AIC and the second term is intended to penalize complex models by increasing the value of AIC. Since this does not use the posterior distribution, it does not take into account any information regarding the uncertainty of the parameter.</p>
</div>
<div class="section" id="bayesian-information-criterion-bic">
<h3>Bayesian Information Criterion (BIC)<a class="headerlink" href="#bayesian-information-criterion-bic" title="Permalink to this headline">¶</a></h3>
<p>The BIC is very similar to the AIC (and in fact not very Bayesian at all). The first term is identical to that in the AIC, however the bias correction term now incorporates the number of samples as well.</p>
<div class="math notranslate nohighlight">
\[BIC = -2 \sum_{i=1}^n log p(y_i | \theta_{mle}) + n_{parameters} \; log \; n_{samples}\]</div>
</div>
<div class="section" id="deviance-information-criterion-dic">
<h3>Deviance Information Criterion (DIC)<a class="headerlink" href="#deviance-information-criterion-dic" title="Permalink to this headline">¶</a></h3>
<p>The DIC is a more Bayesian alternative that uses the posterior mean point estimate \(\theta_{Bayes}\) instead of the MLE estimate. Here \(\theta_{Bayes}\) is the expected value of \(\theta\).</p>
<div class="math notranslate nohighlight">
\[DIC = -2 \sum_{i=1}^n log p(y_i | \theta_{Bayes}) + 2 var_{posterior} \; log p(y_i | \theta )\]</div>
</div>
<div class="section" id="widely-applicable-information-criterion-waic">
<h3>Widely Applicable Information Criterion (WAIC)<a class="headerlink" href="#widely-applicable-information-criterion-waic" title="Permalink to this headline">¶</a></h3>
<p><strong>Reference</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="http://www.stat.columbia.edu/~gelman/research/published/waic_understand3.pdf">WAIC by Gelman</a></p></li>
</ul>
<p>The Widely Applicable Information Criterion or WAIC is a Bayesian extension to the AIC. The derivation for the log pointwise predictive density is similar to what we covered above, but is replicated here to keep it consistent with the paper referenced.</p>
<div class="section" id="log-pointwise-predictive-density-lppd">
<h4>Log pointwise predictive density (lppd)<a class="headerlink" href="#log-pointwise-predictive-density-lppd" title="Permalink to this headline">¶</a></h4>
<p>The predicted value of a new data point <span class="math notranslate nohighlight">\(y_{new}\)</span> can be defined</p>
<div class="math notranslate nohighlight">
\[p_{post}(y_{new}) = \int p(y_{new} | \theta) p_{post}(\theta) d \theta \]</div>
<p>If we take the log of both sides we get</p>
<div class="math notranslate nohighlight">
\[log p_{post}(y_{new}) = log \int p(y_{new} | \theta) p_{post}(\theta) d \theta\]</div>
<p>where \(p_{post}(\theta)\) is the posterior distribution of \(\theta\) obtained by training our model. This is the predictive fit of the new point. If we have a number of new data points i=1,…n we can write the following for the log pointwise predictive density for a model using the new data</p>
<div class="math notranslate nohighlight">
\[lppd = log \prod_i p_{post} (y_{new_i}) = \sum_i \int log p(y_{new_i} | \theta ) p_{post} (\theta) d \theta\]</div>
<ul class="simple">
<li><p>In practice, the inner integral over \(\theta\) is computed using an average over possible values of \(\theta\) (sampled) denoted as \(\theta_S\).</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\sum_i \int log p(y_{new_i} | \theta ) p_{post} (\theta) d \theta = \sum_i log \dfrac{1}{S} \sum_S p(y_{new_i} | \theta_{S})\]</div>
<ul class="simple">
<li><p>Now suppose we don’t have a holdout set <span class="math notranslate nohighlight">\(y_{new}\)</span> and we compute the lppd over our training set, that is not a good measure for future performance of the model. So the WAIC adds a term to correct for this overestimated performance. This correction measures the variance of the log-likelihood for each element of y computed over the different samples of \(\theta_S\). This correction can be seen as a type of penalization intended to reduce the number of parameters since more model parameters imply larger spread or variance of the posterior.</p></li>
</ul>
<div class="math notranslate nohighlight">
\[2 \cdot \sum_i Var_{s} ( log p(y_{new_i} | \theta_{S}) )\]</div>
<ul class="simple">
<li><p>WAIC is now defined as the sum of the two terms above</p></li>
</ul>
<div class="math notranslate nohighlight">
\[WAIC = -2 \sum_i log \dfrac{1}{S} \sum_S p(y_{new_i} | \theta_{S}) +  2 \sum_i Var_{s} ( log p(y_{new_i} | \theta_{S}) )\]</div>
</div>
</div>
<div class="section" id="a-qualitative-discussion">
<h3>A Qualitative Discussion<a class="headerlink" href="#a-qualitative-discussion" title="Permalink to this headline">¶</a></h3>
<ul class="simple">
<li><p>The AIC may not work as well for more complex models since this just uses the number of parameters to penalize the model</p></li>
<li><p>It is worth emphasizing here that all the metrics/methods above, with the exception of Cross-Validation (with a test set) use in-sample data to assess out-of-sample performance. This is analogous to using the training set instead of a test set to evaluate the model performance in Machine Learning. However, unlike what is normally performed in Machine Learning, we apply a bias correction to correct for the error introduced by estimating the performance on the in-sample data. However, Cross Validation eliminates the need for this error correction altogether if a test set is used.</p></li>
<li><p>AIC and DIC are easier to compute but they are not fully Bayesian unlike the WAIC. However, the WAIC is more computationally intensive to calculate.</p></li>
<li><p>Cross-Validation can be made Bayesian by computing the log posterior density. However, this becomes quite expensive to compute compared to the other techniques.</p></li>
</ul>
</div>
</div>
<div class="section" id="entropy-and-kl-divergence">
<h2>Entropy and KL Divergence<a class="headerlink" href="#entropy-and-kl-divergence" title="Permalink to this headline">¶</a></h2>
<p><strong>Reference</strong></p>
<ul class="simple">
<li><p><a class="reference external" href="https://arxiv.org/pdf/1511.00860.pdf">Information Theory</a></p></li>
</ul>
<p>I am using summation in the examples below assuming discrete distributions. This can be replaced by the integral for continuous distributions.</p>
<div class="section" id="entropy">
<h3>Entropy<a class="headerlink" href="#entropy" title="Permalink to this headline">¶</a></h3>
<p>If there is a random discrete variable ‘x’ with a probability distribution given by P(x), the entropy of the random variable ‘x’ is a measure of information uncertainty, which can be computed as</p>
<div class="math notranslate nohighlight">
\[H(x) = - \sum_x \: p(x) log \:  p(x) \]</div>
<p>where a larger value of entropy indicates higher uncertainty. Geometrically, we can visualize this as the distribution having a larger spread. While is easy to equate this with variance, there are examples such as a Bimodal Gaussian distribution where increasing the variance does not increase the entropy. Entropy is a measure of the mass of probability density around a point whereas variance measures how far the probability mass extends from the mean. It is possible to have two narrow modes that are far apart which would indicate a high variance but low entropy due to the relative certainty of the values (around the narrow modes).</p>
<p>Entropy is a useful way to define priors since a prior with high entropy can be used as an uninformative prior. Given certain constraints on the parameter values, the following can be used as priors for those parameters.</p>
<ol class="simple">
<li><p>No constraints - Uniform distribution</p></li>
<li><p>Positive mean with regions of high density - Exponential distribution</p></li>
<li><p>Fixed variance  - Normal distribution</p></li>
<li><p>Two outcomes with a fixed mean - Binomial distribution</p></li>
</ol>
<p>The following code shows the entropy of different distributions</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1"># Example from [1]</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span> 
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">stats</span>
<span class="kn">from</span> <span class="nn">matplotlib</span> <span class="kn">import</span> <span class="n">pyplot</span> <span class="k">as</span> <span class="n">plt</span>
<span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">seed</span><span class="p">(</span><span class="mi">912</span><span class="p">)</span>
<span class="n">x</span> <span class="o">=</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">q</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">0.75</span><span class="p">)</span>
<span class="n">q2</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">binom</span><span class="p">(</span><span class="mi">10</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">r</span> <span class="o">=</span> <span class="n">stats</span><span class="o">.</span><span class="n">randint</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>
<span class="n">true_distribution</span> <span class="o">=</span> <span class="p">[</span><span class="nb">list</span><span class="p">(</span><span class="n">q</span><span class="o">.</span><span class="n">rvs</span><span class="p">(</span><span class="mi">200</span><span class="p">))</span><span class="o">.</span><span class="n">count</span><span class="p">(</span><span class="n">i</span><span class="p">)</span> <span class="o">/</span> <span class="mi">200</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">x</span><span class="p">]</span>
<span class="n">q_pmf</span> <span class="o">=</span> <span class="n">q</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">q2_pmf</span> <span class="o">=</span> <span class="n">q2</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">r_pmf</span> <span class="o">=</span> <span class="n">r</span><span class="o">.</span><span class="n">pmf</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
<span class="n">fig</span><span class="p">,</span> <span class="n">ax</span> <span class="o">=</span> <span class="n">plt</span><span class="o">.</span><span class="n">subplots</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">3</span><span class="p">,</span> <span class="n">figsize</span><span class="o">=</span><span class="p">(</span><span class="mi">20</span><span class="p">,</span><span class="mi">5</span><span class="p">))</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">q_pmf</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;entropy = </span><span class="si">{</span><span class="n">stats</span><span class="o">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">q_pmf</span><span class="p">)</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">r_pmf</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;entropy = </span><span class="si">{</span><span class="n">stats</span><span class="o">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">r_pmf</span><span class="p">)</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">vlines</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="n">q2_pmf</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="sa">f</span><span class="s1">&#39;entropy = </span><span class="si">{</span><span class="n">stats</span><span class="o">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">q2_pmf</span><span class="p">)</span><span class="si">:</span><span class="s1">.2f</span><span class="si">}</span><span class="s1">&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Binomial distribution with parameter 0.75&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Random distribution&#39;</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">set_title</span><span class="p">(</span><span class="s1">&#39;Binomial disttribution with parameter 0.5&#39;</span><span class="p">)</span>
<span class="n">fig</span><span class="o">.</span><span class="n">suptitle</span><span class="p">(</span><span class="s2">&quot;Entropy of different distributions&quot;</span><span class="p">)</span>
<span class="n">stats</span><span class="o">.</span><span class="n">entropy</span><span class="p">(</span><span class="n">true_distribution</span><span class="p">)</span>
<span class="c1">#ax[idx].set_xticks(x)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">handlelength</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">handlelength</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
<span class="n">ax</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">handlelength</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>&lt;matplotlib.legend.Legend at 0x7fb442937df0&gt;
</pre></div>
</div>
<img alt="../_images/BayesianInference_6_1.png" src="../_images/BayesianInference_6_1.png" />
</div>
</div>
</div>
</div>
<div class="section" id="ungraded-evaluation-15-mins">
<h2>UNGRADED EVALUATION (15 mins)<a class="headerlink" href="#ungraded-evaluation-15-mins" title="Permalink to this headline">¶</a></h2>
<p>Try the above with different parameters for</p>
<ol class="simple">
<li><p>The Binomial distribution</p></li>
<li><p>The Poisson distribution</p></li>
</ol>
<div class="section" id="kl-divergence">
<h3>KL Divergence<a class="headerlink" href="#kl-divergence" title="Permalink to this headline">¶</a></h3>
<p>KL Divergence is similar to the concept of entropy except that it is used to compare the similarity and closeness of two distributions. It is defined for two discrete distributions as</p>
<div class="math notranslate nohighlight">
\[KL(p||q) = \sum_x p(x) \: log \dfrac{p(x)}{q(x)}\]</div>
<p>The value of the KL Divergence varies from 0 for identical distributions to infinity depending on the dissimilarity between the distributions. It is important to understand that this is not a distance metric as it is not symmetric, since</p>
<div class="math notranslate nohighlight">
\[KL(q||p) = \sum_x q(x) \: log \dfrac{q(x)}{p(x)}\]</div>
<p>The Jensen Shannon Divergence is the symmetric version of the KL Divergence</p>
<p>The KL Divergence <span class="math notranslate nohighlight">\(KL(p||q)\)</span> can be seen as the difference of two entropies</p>
<div class="math notranslate nohighlight">
\[KL(p||q) = \sum_x p(x) \: log p(x) - \sum_x p(x) \: log q(x)\]</div>
<p>where the first term is the entropy of ‘p’ and the second term is the cross-entropy between ‘p’ and ‘q’.</p>
<p>The KL Divergence is commonly used in Machine Learning to learn a distribution. If the true distribution was available, the proposed distribution can be optimized to make it as close to the true distribution as possible.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">entropy</span>

<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;KL Divergence between the true distribution and the uniform distribution &quot;</span><span class="p">,</span><span class="n">entropy</span><span class="p">(</span><span class="n">true_distribution</span><span class="p">,</span> <span class="n">r_pmf</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;KL Divergence between the true distribution and the q distribution &quot;</span><span class="p">,</span><span class="n">entropy</span><span class="p">(</span><span class="n">true_distribution</span><span class="p">,</span> <span class="n">q_pmf</span><span class="p">))</span>
<span class="nb">print</span><span class="p">(</span><span class="s2">&quot;KL Divergence between the true distribution and the q2 distribution &quot;</span><span class="p">,</span><span class="n">entropy</span><span class="p">(</span><span class="n">true_distribution</span><span class="p">,</span> <span class="n">q2_pmf</span><span class="p">))</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>KL Divergence between the true distribution and the uniform distribution  0.7394593875511319
KL Divergence between the true distribution and the q distribution  0.009657896086383405
KL Divergence between the true distribution and the q2 distribution  1.276465607901914
</pre></div>
</div>
</div>
</div>
</div>
</div>
<div class="section" id="ungraded-evaluation-1-hr">
<h2>UNGRADED EVALUATION (1 hr)<a class="headerlink" href="#ungraded-evaluation-1-hr" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>Write Python code to compute the KL divergence between two distributions. Use two normal discrete distributions a and b with different mean and variance. Note how the KL divergence changes as the number of samples in the distribution increases.</p></li>
<li><p>Compute the KL Divergence between a and b - KL(a||b)</p></li>
<li><p>Compute the KL Divergence between b and a - KL(b||a)</p></li>
<li><p>Compute the KL Divergence between a and itself</p></li>
<li><p>Compute the KL Divergence between a and the shifted version of a. Note how the KL Divergence varies as the shifted version moves away.</p></li>
</ol>
</div>
<div class="section" id="model-averaging">
<h2>Model Averaging<a class="headerlink" href="#model-averaging" title="Permalink to this headline">¶</a></h2>
<p>Sometimes, model selection may not the most appropriate solution for our problem, e.g. when none of the individual models provide satisfactory performance. Or, as mentioned below when we want to utilize the variance associated with the different models as a measure of uncertainty. In these cases we may want to resort to model averaging.</p>
<div class="section" id="pseudo-bayesian-modeling-averaging">
<h3>Pseudo Bayesian Modeling Averaging<a class="headerlink" href="#pseudo-bayesian-modeling-averaging" title="Permalink to this headline">¶</a></h3>
<p><a class="reference external" href="http://www.stat.columbia.edu/~gelman/research/published/stacking_paper_discussion_rejoinder.pdf">Using Stacking to Average Bayesian Predictive Distributions</a></p>
<p>When there are several models that one can chose from, it is tempting to pick the one with the best performance (depending on how we define performance). However, in doing so we are ignoring the uncertainty information provided by the other models. One way to mitigate this uncertainty is by performing model averaging. The meta-model obtained by using a weighted average of all the models can be used to make predictions. One way that this averaging is done is by computing the weights similar to using a softmax formula</p>
<div class="math notranslate nohighlight">
\[w_i = \dfrac{e^{-dE_i / 2}}{\sum_j e^{-dE_j / 2}}\]</div>
<p>where \(dE_i\) is the difference in the WAIC value of the i’th model compared to the model with the lowest WAIC.</p>
<p>Any Information Criterion metric can be used in this equation such as the AIC. Averaging the models using the weights computed this way is called pseudo Bayesian Modeling Averaging.</p>
</div>
<div class="section" id="stacking">
<h3>Stacking<a class="headerlink" href="#stacking" title="Permalink to this headline">¶</a></h3>
<p>Another technique that was proposed recently is the stacking of predictive distributions. The idea behind this is to combine models such that you minimize the divergence between the weighted metamodel and the true model. When a logarithmic score is used, similar to a KL Divergence, the following equation can be used</p>
<div class="math notranslate nohighlight">
\[model = max_w \dfrac{1}{n} \sum_i^{n} log \sum_k w_k p(y_i | y_{-i}, M_k)\]</div>
<p>where n is the number of data points and \(M_k\) is the k’th model and \(w_k\) is the weight applied to the k’th model. \(y_{-i}\) is every element in y except \(y_i\). The term \(p(y_i | y_{-i}, M_k)\) corresponds to the predictive probability density using a Leave-One-Out Cross-Validation (LOOCV) procedure. The goal is to select the combination of weights that maximizes the probability of seeing \(y_i\), thereby giving us the ideal metamodel that minimizes the divergence, to the best of our knowledge, based on the data available. Note here that argmax is computed over ‘w’ as opposed to ‘n’ as it listed in some resources.</p>
</div>
</div>
<div class="section" id="graded-evaluation-30-mins">
<h2>GRADED EVALUATION (30 mins)<a class="headerlink" href="#graded-evaluation-30-mins" title="Permalink to this headline">¶</a></h2>
<ol>
<li><p>Underfitting is bad because</p>
<p>a. It cannot capture complex behavior and will have inherent error (</p>
<p>b. The predicted value is always less than the true value</p>
</li>
<li><p>Overfitting is bad because</p>
<p>a. The model that is overfit will learn noise</p>
<p>b. The model is too big</p>
</li>
<li><p>Variance of a model is related to</p>
<p>a. A model’s ability to adapt its parameters to training data</p>
<p>b. The sensitivity of the model to the inputs</p>
</li>
<li><p>AIC is a primarily a non-Bayesian metric</p>
<p>a. True</p>
<p>b. False</p>
</li>
<li><p>KL Divergence is a distance metric</p>
<p>a. True</p>
<p>b. False</p>
</li>
<li><p>The symmetric version of the KL Divergence is</p>
<p>a. Jenson Button Divergence</p>
<p>b. Jensen Shannon Divergence</p>
</li>
<li><p>Entropy is a measure of</p>
<p>a. Information symmetry</p>
<p>b. Information uncertainty</p>
</li>
<li><p>The WAIC is the Bayesian extension to the AIC</p>
<p>a. True</p>
<p>b. False</p>
</li>
<li><p>For Deviance of models, a well-fit model has a value</p>
<p>a. Infinity</p>
<p>b. Close to 0</p>
</li>
<li><p>The value of <span class="math notranslate nohighlight">\(R^2\)</span> for a model that perfectly fits the data is</p></li>
</ol>
<p>a. 1</p>
<p>b. 0</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "sjster/statistical_computing_book",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
        <div class='prev-next-bottom'>
            

        </div>
        
        </div>
    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Srijith Rajamohan, Ph.D.<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>
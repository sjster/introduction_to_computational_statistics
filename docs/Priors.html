
<!DOCTYPE html>

<html xmlns="http://www.w3.org/1999/xhtml">
  <head>
    <meta charset="utf-8" />
    <title>Priors &#8212; Introduction to Computational Statistics with PyMC3</title>
    
  <link rel="stylesheet" href="../_static/css/index.f658d18f9b420779cfdf24aa0a7e2d77.css">

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      
  <link rel="stylesheet"
    href="../_static/vendor/open-sans_all/1.44.1/index.css">
  <link rel="stylesheet"
    href="../_static/vendor/lato_latin-ext/1.44.1/index.css">

    
    <link rel="stylesheet" href="../_static/sphinx-book-theme.40e2e510f6b7d1648584402491bb10fe.css" type="text/css" />
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.d3f166471bb80abb5163.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/language_data.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.d31b09fe5c1d09cb49b26a786de4a05d.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebelab@latest/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <link rel="next" title="Bayesian vs. Frequentist Statistics" href="Intro.html" />
    <link rel="prev" title="Inference and Decisions" href="Decisions.html" />

    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />



  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
<a class="navbar-brand text-wrap" href="../index.html">
  
  <img src="../_static/logo_large.png" class="logo" alt="logo">
  
  
  <h1 class="site-logo" id="site-title">Introduction to Computational Statistics with PyMC3</h1>
  
</a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form>
<nav class="bd-links" id="bd-docs-nav" aria-label="Main navigation">
    <p class="caption collapsible-parent">
 <span class="caption-text">
  Getting started
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="getting_started.html">
   Setting up Your Python Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Databricks.html">
   Introduction to the Databricks Environment
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Introduction to Bayesian Statistics
 </span>
</p>
<ul class="current nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="Foundations.html">
   Foundations of Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="CentralTendency.html">
   Distributions, Central Tendency, and Shape Parameters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="ParameterEstimation.html">
   Parameter Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Bayes.html">
   Introduction to the Bayes Theorem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Decisions.html">
   Inference and Decisions
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Priors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Intro.html">
   Bayesian vs. Frequentist Statistics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Distributions.html">
   Introduction to Common Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Sampling.html">
   Sampling Strategies
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  Introduction to Monte Carlo Methods
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="BayesianInference.html">
   Topics in Model Performance
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MonteCarlo.html">
   Introduction to Monte Carlo Methods
  </a>
 </li>
</ul>
<p class="caption collapsible-parent">
 <span class="caption-text">
  PyMC3 for Bayesian Modeling and Inference
 </span>
</p>
<ul class="nav sidenav_l1">
 <li class="toctree-l1">
  <a class="reference internal" href="PyMC3.html">
   Introduction to PyMC3
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Reparameterization.html">
   Reparameterization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Covid_modeling.html">
   Covid Modeling with PyMC3
  </a>
 </li>
</ul>

</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="row topbar fixed-top container-xl">
    <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show">
    </div>
    <div class="col pl-2 topbar-main">
        
        <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
            data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
            aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
            title="Toggle navigation" data-toggle="tooltip" data-placement="left">
            <i class="fas fa-bars"></i>
            <i class="fas fa-arrow-left"></i>
            <i class="fas fa-arrow-up"></i>
        </button>
        
        
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/docs/Priors.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

        <!-- Source interaction buttons -->


        <!-- Full screen (wrap in <a> to have style consistency -->
        <a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
                data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
                title="Fullscreen mode"><i
                    class="fas fa-expand"></i></button></a>

        <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sjster/statistical_computing_book/master?urlpath=tree/mini_book/docs/Priors.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/sjster/statistical_computing_book/blob/master/mini_book/docs/Priors.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

    </div>

    <!-- Table of contents -->
    <div class="d-none d-md-block col-md-2 bd-toc show">
        
        <div class="tocsection onthispage pt-5 pb-3">
            <i class="fas fa-list"></i> Contents
        </div>
        <nav id="bd-toc-nav">
            <ul class="nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prior-likelihood">
   Prior * Likelihood
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conjugate-priors">
     Conjugate priors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#noninformative-priors">
     Noninformative priors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#jeffrey-s-non-informative-prior">
     Jeffrey’s non-informative prior
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#informative-priors">
     Informative priors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weakly-informative-or-vague-priors">
     Weakly informative or vague priors
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#senstivity-analysis-of-priors">
   Senstivity analysis of priors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#likelihood-prior">
   Likelihood * prior
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-binomial-beta">
     Example binomial-beta
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#ungraded-evaluation-15-min">
   UNGRADED EVALUATION (15 min)
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id1">
     1.
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#id2">
     2.
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graded-evaluation-30-mins">
   GRADED EVALUATION (30 mins)
  </a>
 </li>
</ul>

        </nav>
        
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="priors">
<h1>Priors<a class="headerlink" href="#priors" title="Permalink to this headline">¶</a></h1>
<p>Bayes’ Rule gives us a method to update our beleifs based on prior knowledge.  In words, this looks like:</p>
<p><span class="math notranslate nohighlight">\(posterior = \frac{likelihood \ast prior}{marginal}\)</span></p>
<p>Which can also be stated as:</p>
<p><span class="math notranslate nohighlight">\(posterior = \frac{likelihood \ast prior}{evidence}\)</span></p>
<p>Or, being explicit about data <span class="math notranslate nohighlight">\(\mathcal{D}\)</span> and parameters <span class="math notranslate nohighlight">\(\theta\)</span>, we might write it as:</p>
<p><span class="math notranslate nohighlight">\(p(\theta|\mathcal{D}) = \frac{p(\mathcal{D}|\theta) \ast p(\theta)}{p(\mathcal{D})}\)</span></p>
<p>This is a change in frame of reference with respect to variable data and fixed parameters which is the hallmark of Bayesian thinking.  After you observe the data, it is fixed.  This session we will talk about priors, how to think about the prior and what influence it has on the posterior.</p>
<p>General references:</p>
<ul class="simple">
<li><p>Pattern Recognition and Machine Learning (9780387310732) Bishop, Christopher M.</p></li>
<li><p>Statistical Inference (9780534243128): Casella, George, Berger, Roger L.</p></li>
<li><p>Probability Theory and Statistical Inference: Empirical Modeling with Observational Data (9781107185142): Spanos, A.</p></li>
<li><p>Bayesian Models: A Statistical Primer for Ecologists (9780691159287): Hobbs, N. Thompson, Hooten, Mevin B.</p></li>
<li><p>A First Course in Bayesian Statistical Methods (0387922997): Hoff, Peter D.</p></li>
</ul>
<br>
<br>
<hr style="border:2px solid blue"> </hr><div class="section" id="prior-likelihood">
<h2>Prior * Likelihood<a class="headerlink" href="#prior-likelihood" title="Permalink to this headline">¶</a></h2>
<p>In learning about Bayes’ Rule, we saw how our prior information about the occurance of an event is updated through the conditional probability of an event.  We are now considering the case where both the conditional probability of data|parameters and the prior are distributions (including possibly the uniform).</p>
<p>As related to Bayes’ Rule, the prior is the unconditional probability of the parameters before (new) data.  Priors can come from a number of sources including</p>
<ul class="simple">
<li><p>past experiments or experience</p></li>
<li><p>some sort of desire for balance or weighting in a decision</p></li>
<li><p>non-informative, but objective</p></li>
<li><p>mathmatical convenience</p></li>
</ul>
<p>Choice of the prior is as much about what is currently known about the parameters as it is about the goal of the analysis.  This makes the choice of prior subjective and often contested.  How do we choose priors?  Two broad categories could include:</p>
<ul class="simple">
<li><p>noninformative</p></li>
<li><p>informative</p></li>
</ul>
<p>The priors can also be <strong>proper</strong>, ie conform to the rules of probability and integrate to 1, or <strong>improper</strong>.  As an example, consider the uniform distribution as a prior to the mean in a normal distribution, we want to apply equal weights to all possiblle values of <span class="math notranslate nohighlight">\(\mu\)</span>.  However, <span class="math notranslate nohighlight">\(\mu \in [-\infty,\infty]\)</span> and our uniform prior must span the same range. Jeffrey’s prior is another example of an (often) improper prior discussed below.</p>
<p>The form of the prior is also a choice.  Convenient choices of priors can lead to closed form solutions for the posterior.</p>
<br>
<br>
<hr style="border:2px solid blue"> </hr>
<div class="section" id="conjugate-priors">
<h3>Conjugate priors<a class="headerlink" href="#conjugate-priors" title="Permalink to this headline">¶</a></h3>
<p>Conjugate priors are priors that induce a known distribution in the posterior.  When computing the posterior probability, if we have a justifiable reason for using pairing the likelihood which a conjugate prior, we will find the posterior probabality is a known distribution.  For example, consider</p>
<p><span class="math notranslate nohighlight">\(X \sim Bern(\theta)\)</span></p>
<p>The likelihood takes the form:</p>
<p><span class="math notranslate nohighlight">\(f(x|\theta) = \prod_{i=1}^n \theta^{x_i}(1-\theta)^{1-x_i} = \theta^{k}(1-\theta)^{n-k}\)</span>, where <span class="math notranslate nohighlight">\(k=\sum x_i\)</span>,shifting our focus from x, to <span class="math notranslate nohighlight">\(\theta\)</span>, if <span class="math notranslate nohighlight">\(\theta\)</span> were the random variable, it appears to have the functional kernel of the distribution of a beta.  If we assume the prior take the form of a beta distribution (dropping all the constants):</p>
<p><span class="math notranslate nohighlight">\(f(x|\theta) \ast p(\theta) \propto \theta^{k}(1-\theta)^{n-k} \ast \theta^{\alpha-1}(1-\theta)^{\beta-1} = \theta^{\alpha+k-1}(1-\theta)^{\beta+n-k-1}\)</span></p>
<p>Which we recognize as <span class="math notranslate nohighlight">\(Beta(\alpha+k,\beta+n-k)\)</span> in <span class="math notranslate nohighlight">\(\theta\)</span>.</p>
<p>IF, we are modelling a Bernoulli process and use a prior with a Beta distribution, our posterior has a known distribution with parameters as shown.</p>
<p>Common conjugate priors by likelihood type:<br />
likelihood - prior<br />
Binomial - beta<br />
Poisson - Gamma<br />
Normal (known <span class="math notranslate nohighlight">\(\sigma^2\)</span>) - Normal<br />
Normal (known <span class="math notranslate nohighlight">\(\mu\)</span>) - inverse-gamma<br />
Multivariate normal (known <span class="math notranslate nohighlight">\(\Sigma\)</span>) - inverse-Wishart</p>
<br>
<br>
<hr style="border:2px solid blue"> </hr>
</div>
<div class="section" id="noninformative-priors">
<h3>Noninformative priors<a class="headerlink" href="#noninformative-priors" title="Permalink to this headline">¶</a></h3>
<p>Noninformative priors are priors that suggest ignorance as to the parameters.  These are sometimes called vague or diffuse priors.  The priors generally cover the region of the parameter space relatively smoothly.  Common noninformative priors include <span class="math notranslate nohighlight">\(unif(-1000,1000)\)</span>, <span class="math notranslate nohighlight">\(N(0,10,000)\)</span>.  Note that seeminly vague priors can actually be strongly informative.  Consider the case of modeling a binary model for y following a Bernoulli distribution.  A common modeling technique would be to transform the problem using the logit function.  For instance:</p>
<p><span class="math notranslate nohighlight">\(y \sim Bern(p)\)</span><br />
<span class="math notranslate nohighlight">\(p=logit^{-1}(\beta_0 + \beta_1)\)</span></p>
<p>Placing priors <span class="math notranslate nohighlight">\([\beta_0,\beta_1] \sim N(0,100)\)</span> places undue weight on 0 through the transform while using a weakly informative prior <span class="math notranslate nohighlight">\([\beta_0,\beta_1] \sim N(0,2^2)\)</span> gives a more diffuse effect on the parameter posterior.</p>
<br>
<br>
<hr style="border:2px solid blue"> </hr>
</div>
<div class="section" id="jeffrey-s-non-informative-prior">
<h3>Jeffrey’s non-informative prior<a class="headerlink" href="#jeffrey-s-non-informative-prior" title="Permalink to this headline">¶</a></h3>
<p>The Jeffrey’s prior is a non-informative prior that is derived from the Fisher information.  This prior is non-informative in that we do not specify prior information, but it is informative in that we use the data to information to shape the prior.  Specifically, the Fisher’s information tells us how much information about <span class="math notranslate nohighlight">\(\theta\)</span> is included in the data.  Formally, the Jeffrey’s prior is derived by:</p>
<p><span class="math notranslate nohighlight">\(p(\theta) \propto \sqrt{I_n(\theta)}\)</span>, where</p>
<p><span class="math notranslate nohighlight">\(I_n(\theta) = E_{\theta}\big\{\big(\frac{\partial ln f(\theta)}{\partial \theta}\big)\big\} = -E_{\theta}\big\{\big(\frac{\partial^2 ln L(\theta)}{\partial \theta^2}\big)\big\}\)</span></p>
<p>This is a <span class="math notranslate nohighlight">\(pxp\)</span> matrix of partial derivatives when there are p parameters.</p>
<p>Working through an example, let</p>
<p><span class="math notranslate nohighlight">\(X \sim gamma(\alpha,\beta)\)</span>, assuming <span class="math notranslate nohighlight">\(\alpha\)</span> is known and <span class="math notranslate nohighlight">\(\beta\)</span> is unknown.  Through the likelihood, one can work out the Fisher’s information to be</p>
<p><span class="math notranslate nohighlight">\(I_n(\beta) = \frac{n\alpha}{\beta^2}\)</span> leading to the Jeffrey’s prior for <span class="math notranslate nohighlight">\(\beta\)</span>:</p>
<p><span class="math notranslate nohighlight">\(p(\beta) \propto \sqrt{\frac{n\alpha}{\beta^2}} \propto 1\)</span></p>
<p>Note that Jeffrey’s priors are not guaranteed to be proper.  Perhaps most importantly, Jeffrey’s priors are stable under reparameterization.</p>
<br>
<br>
<hr style="border:2px solid blue"> </hr>
</div>
<div class="section" id="informative-priors">
<h3>Informative priors<a class="headerlink" href="#informative-priors" title="Permalink to this headline">¶</a></h3>
<p>Informative priors are explicitly chosen to represent current knowledge or belief about the parameters of interest.  When choosing informative priors, one can also choose the form of the prior.  As an example, suppose we are back at tossing coins.  We were given a new coin from a friend and were told it would generate heads with P(heads) = 0.75.  We conduct a new experiment to characterize the distribution of <span class="math notranslate nohighlight">\(\theta\)</span>.  We will see that a computationally convenient distribution on the prior is a beta(a,b) when dealing with Bernoulli trials.  What do we choose for paramters the beta distribution?  To incorporate the information we have, we might choose a beta having a mean close to 0.75 to represent the information given to us.  We also have the ability to choose the precision/scale.  We can tune that to represent some amount of disbelief in the un-fairness of the coin.  Looking at the plots below, we could use the beta(6.9,3) or beta(16,6) distribution as examples.  These were chosen empirically through plotting and calculating the mean and modes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">y1</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y3</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">y4</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">6.9</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">y5</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;1/2,1/2&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;1,1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y3</span><span class="p">,</span> <span class="s2">&quot;g--&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;3,3&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y4</span><span class="p">,</span> <span class="s2">&quot;b--&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;6.9,3&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y5</span><span class="p">,</span> <span class="s2">&quot;y--&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;16,6&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Priors_2_0.png" src="../_images/Priors_2_0.png" />
</div>
</div>
</div>
<div class="section" id="weakly-informative-or-vague-priors">
<h3>Weakly informative or vague priors<a class="headerlink" href="#weakly-informative-or-vague-priors" title="Permalink to this headline">¶</a></h3>
<p>In the plots above, we see that we can tune our prior belief using the mean or even mode to center our beleif and variance as a measure of our strength of belief.  For the beta,(6.9,3) this may look like:</p>
<p><span class="math notranslate nohighlight">\(E[x] = \frac{a}{a+b} = 0.70\)</span><br />
<span class="math notranslate nohighlight">\(mode(x) = \frac{a-1}{(a+b-2)} = 0.77\)</span><br />
<span class="math notranslate nohighlight">\(Var(x) = \frac{ab}{(a+b)^2(a+b+1)} = 0.05\)</span></p>
<p>Tuning the prior to include slightly more confidence in the prior information may suggest a beta(16,6) which reduces the variance to 0.028.  These priors are vague in that the mass of the prior is still diffuse allowing the data to drive the posterior through the likelihood.</p>
<br>
<br>
<hr style="border:2px solid blue"> </hr>
</div>
</div>
<div class="section" id="senstivity-analysis-of-priors">
<h2>Senstivity analysis of priors<a class="headerlink" href="#senstivity-analysis-of-priors" title="Permalink to this headline">¶</a></h2>
<p>The general approach to using priors in models is to start with some justification for a prior, run the analysis, then come up with competing priors and reexamine the conclusions under the alternative priors.  The results of the final model and the analysis of the sensitivity of the analysis to the choice of prior are written up as a package.</p>
<p>For a discussion of steps and methods to use in a sensitivity analysis, see:
Bayesian Data Analysis by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin.</p>
<br>
<br>
<hr style="border:2px solid blue"> </hr>
</div>
<div class="section" id="likelihood-prior">
<h2>Likelihood * prior<a class="headerlink" href="#likelihood-prior" title="Permalink to this headline">¶</a></h2>
<p>What we have been talking about is priors and thier influence on the likelihood in development of the posterior.  A few examples should help.</p>
<br>
<br>
<hr style="border:2px solid blue"> </hr>
<div class="section" id="example-binomial-beta">
<h3>Example binomial-beta<a class="headerlink" href="#example-binomial-beta" title="Permalink to this headline">¶</a></h3>
<p>Let us first consider the coin toss experiment above, as a reminder, we are given a presumable unfair coin with p(heads)=0.75.  Let us collect new data and analyze the posterior.  First, we are going to collect a series of tosses, so the likelihood is best given by a binomial distribution.  Examining the likelihood from the perspective of the parameter, <span class="math notranslate nohighlight">\(\theta\)</span>, we note both that the functional form of the likelihood appears to be that of a beta and the support of a beta, [0,1], vs the valid values of <span class="math notranslate nohighlight">\(\theta\)</span> match.</p>
<p><span class="math notranslate nohighlight">\(p(\theta|x) = f(x|\theta) \ast p(\theta) \propto \big [{n \choose x} \theta^x (1-\theta)^{n-x} \big ] \ast \big [\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1} \big ]\)</span></p>
<p>Simplifying a bit, we get:</p>
<p><span class="math notranslate nohighlight">\(p(\theta|x) \propto \theta^{\alpha+x-1} (1-\theta)^{\beta+n-x-1}\)</span></p>
<p>Following along the discussion above, we note that the things we have that are tunable (hyperparameters), are <span class="math notranslate nohighlight">\(\alpha\)</span> and <span class="math notranslate nohighlight">\(\beta\)</span>.  Here, we are using the value of <span class="math notranslate nohighlight">\(\theta\)</span>, but do to what extent do we believe the friend who gave us the coin and the tip?  Let us simulate three scenarios: a) we don’t believe the friend at all, b) we give some credience to the friend and c) we are strong in our trust.  We will represent these scenarios using beta(1,1), beta(6.9,3),beta(16,6).  Note that we know what the final distribution is, it is a beta(<span class="math notranslate nohighlight">\(\alpha\)</span>+x, <span class="math notranslate nohighlight">\(\beta\)</span>+n-x).  We could simply compute the mean, variance etc.  Instead, let’s visuallize the outcomes.  Assume first, we tossed 10 coins and observed 7 heads.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### assume the coin is as advertized, could have drew from rbinom(p=0.75)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">7</span> <span class="c1">#used x</span>

<span class="n">p1</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">p2</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">6.9</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">p3</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="o">+</span><span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="p">)</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mf">6.9</span><span class="p">,</span> <span class="mi">3</span><span class="o">+</span><span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="p">)</span>
<span class="n">y3</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="o">+</span><span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p1</span><span class="p">,</span> <span class="s2">&quot;b-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;prior = 1,1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p2</span><span class="p">,</span> <span class="s2">&quot;r-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;prior = 6.9,3&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p3</span><span class="p">,</span> <span class="s2">&quot;g-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;prior = 16,6&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="s2">&quot;b--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;posterior = 1,1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;posterior = 6.9,3&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y3</span><span class="p">,</span> <span class="s2">&quot;g--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;posterior = 16,6&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1">## note the distributions all pull towards the likelihood, irrespective of how informative the prior was</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Priors_4_0.png" src="../_images/Priors_4_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### assume the coin is perfectly fair, could have drew from rbinom(p=0.5)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1">#used x</span>

<span class="n">p1</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">p2</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">6.9</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">p3</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="o">+</span><span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="p">)</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mf">6.9</span><span class="p">,</span> <span class="mi">3</span><span class="o">+</span><span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="p">)</span>
<span class="n">y3</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="o">+</span><span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p1</span><span class="p">,</span> <span class="s2">&quot;b-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;prior = 1,1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p2</span><span class="p">,</span> <span class="s2">&quot;r-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;prior = 6.9,3&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p3</span><span class="p">,</span> <span class="s2">&quot;g-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;prior = 16,6&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="s2">&quot;b--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;posterior = 1,1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;posterior = 6.9,3&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y3</span><span class="p">,</span> <span class="s2">&quot;g--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;posterior = 16,6&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1">## again, the likelihood pulls the posterior distribution towards the theta supported by the data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Priors_5_0.png" src="../_images/Priors_5_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### assume the coin is perfectly fair, could have drew from rbinom(p=0.5)</span>
<span class="c1">### BUT, we trusted our friend, what does the size of the experiment do to our posterior</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">n1</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n2</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">n3</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">k1</span> <span class="o">=</span> <span class="mi">5</span> 
<span class="n">k2</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">k3</span> <span class="o">=</span> <span class="mi">25</span>

<span class="n">p1</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">p2</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">6.9</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">p3</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k1</span><span class="o">+</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="o">+</span><span class="n">n1</span><span class="o">-</span><span class="n">k1</span><span class="p">)</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k2</span><span class="o">+</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="o">+</span><span class="n">n2</span><span class="o">-</span><span class="n">k2</span><span class="p">)</span>
<span class="n">y3</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k3</span><span class="o">+</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="o">+</span><span class="n">n3</span><span class="o">-</span><span class="n">k3</span><span class="p">)</span>
<span class="c1">#plt.plot(x, p1, &quot;b-&quot;, label=&quot;prior = 1,1&quot;)</span>
<span class="c1">#plt.plot(x, p2, &quot;r-&quot;,label=&quot;prior = 6.9,3&quot;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p3</span><span class="p">,</span> <span class="s2">&quot;g-&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;prior = 16,6&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="s2">&quot;b--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;n=10&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;n=20&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y3</span><span class="p">,</span> <span class="s2">&quot;g--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;n=50&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../_images/Priors_6_0.png" src="../_images/Priors_6_0.png" />
</div>
</div>
<br>
<br>
<hr style="border:2px solid blue"> </hr>
</div>
</div>
<div class="section" id="ungraded-evaluation-15-min">
<h2>UNGRADED EVALUATION (15 min)<a class="headerlink" href="#ungraded-evaluation-15-min" title="Permalink to this headline">¶</a></h2>
<div class="section" id="id1">
<h3>1.<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h3>
<p>Given a <span class="math notranslate nohighlight">\(y \sim Poisson(\lambda)\)</span> and <span class="math notranslate nohighlight">\(p(\lambda) \sim gamma(a,b)\)</span>, what is the mean of the posterior? <span class="math notranslate nohighlight">\(\bar{y}\)</span> = mean</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>a- a + n$\bar{y}$ (C)  
b- a + $\bar{y}$
</pre></div>
</div>
</div>
<div class="section" id="id2">
<h3>2.<a class="headerlink" href="#id2" title="Permalink to this headline">¶</a></h3>
<p>You are given a data set containing the average weight of male flies and decide to model the average weight via a normal distribution.  You don’t know what to expect in terms of mean, but expect a previous study of female flies to be informative about the variance (<span class="math notranslate nohighlight">\(\sigma_f^2\)</span>).  Using conjugates, your model will be (likelihood <span class="math notranslate nohighlight">\(\ast\)</span> prior):</p>
<p>a- <span class="math notranslate nohighlight">\(N(\mu,\sigma^2) \ast N(\mu_0,\sigma_f^2)\)</span><br />
b- <span class="math notranslate nohighlight">\(N(\mu,\sigma_f^2) \ast N(\mu_0,\sigma_0^2)\)</span> (C)<br />
c- <span class="math notranslate nohighlight">\(N(\mu,\sigma^2) \ast N(\mu,\sigma_0^2)\)</span></p>
</div>
</div>
<div class="section" id="graded-evaluation-30-mins">
<h2>GRADED EVALUATION (30 mins)<a class="headerlink" href="#graded-evaluation-30-mins" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>For a variable <span class="math notranslate nohighlight">\(X \sim N(\mu,\sigma^2)\)</span> where <span class="math notranslate nohighlight">\(\sigma^2\)</span> is fixed and known, find the Jeffrey’s prior. Is it proper?</p></li>
</ol>
<p>a- <span class="math notranslate nohighlight">\(p(\mu) \propto \sqrt{\frac{1}{\sigma^2}} \propto 1\)</span> ; proper (C)</p>
<p>b- <span class="math notranslate nohighlight">\(p(\mu) \propto \sqrt{\frac{1}{\sigma^2}} \propto 1\)</span> ; improper</p>
<p>c- <span class="math notranslate nohighlight">\(p(\mu) \propto \sqrt{\frac{1}{\sigma^2}} \propto \frac{1}{\sigma^2}\)</span> ; proper</p>
<ol class="simple">
<li><p>We are studing a Bernoulli process for which we have no prior information.  We decide to use a non-informative prior such as the beta(1,1).  Because the prior is flat [0,1], this prior will have no effect on the posterior.</p></li>
</ol>
<p>a- true<br />
b- false  (C) .</p>
<ol class="simple">
<li><p>The data we are modelling comes from a geometric distribution.  A good choice of prior family is:</p></li>
</ol>
<p>a- exponential; because there is only one parameter<br />
b- beta; because the for of the exponential matches the kernel of the beta (C)<br />
c- uniform; due to the parameter of the exponential being a proportion</p>
<ol class="simple">
<li><p>Given a prior having mean 10 and data having mean 5, we should expect the posterior mean to lie</p></li>
</ol>
<p>a- to the right of 5 .
b- to the left of 10<br />
c- between 5 and 10 (C) .</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "sjster/statistical_computing_book",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        </div>
    </div>
    
    
    <div class='prev-next-bottom'>
        
    <a class='left-prev' id="prev-link" href="Decisions.html" title="previous page">Inference and Decisions</a>
    <a class='right-next' id="next-link" href="Intro.html" title="next page">Bayesian vs. Frequentist Statistics</a>

    </div>
    <footer class="footer mt-5 mt-md-0">
    <div class="container">
      <p>
        
          By Srijith Rajamohan, Ph.D.<br/>
        
            &copy; Copyright 2020.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>

    
  <script src="../_static/js/index.d3f166471bb80abb5163.js"></script>


    
  </body>
</html>
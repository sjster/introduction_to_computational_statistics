
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Bayesian vs. Frequentist Statistics &#8212; Introduction to Computational Statistics with PyMC3</title>
    
  <link href="../_static/css/theme.css" rel="stylesheet" />
  <link href="../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../" src="../_static/documentation_options.js"></script>
    <script src="../_static/jquery.js"></script>
    <script src="../_static/underscore.js"></script>
    <script src="../_static/doctools.js"></script>
    <script src="../_static/togglebutton.js"></script>
    <script src="../_static/clipboard.min.js"></script>
    <script src="../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../genindex.html" />
    <link rel="search" title="Search" href="../search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../_static/logo_large.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Computational Statistics with PyMC3</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  Getting started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Production/About.html">
   The What, Why and Whom…
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/getting_started.html">
   Setting up Your Python Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Databricks.html">
   Introduction to the Databricks Environment
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction to Bayesian Statistics
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Fundamentals-lecture1-belief-and-probability-unGRADED.html">
   Empirical modeling
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Fundamentals-lecture2-manipulating-probability-unGRADED.html">
   Probability - II
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Fundamentals-lecture3-intro-distributions-unGRADED.html">
   Distributions, central tendency, and shape parameters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Fundamentals-lecture4-MoM-MLE-unGRADED.html">
   Parameter Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Fundamentals-lecture5a-Basics-of-Bayes-unGRADED.html">
   Introduction to the Bayes Theorem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Fundamentals-lecture5b-inference-decisions-unGRADED.html">
   Inference and Decisions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Fundamentals-lecture6-priors-unGRADED.html">
   Priors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Intro.html">
   Bayesian vs. Frequentist Statistics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Distributions.html">
   Introduction to Common Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Sampling.html">
   Sampling Algorithms
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction to Monte Carlo Methods
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Production/BayesianInference.html">
   Topics in Model Performance
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/MonteCarlo.html">
   Introduction to Monte Carlo Methods
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  PyMC3 for Bayesian Modeling and Inference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Production/PyMC3.html">
   Introduction to PyMC3
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Reparameterization.html">
   Centered vs. Non-centered Parameterization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Covid_modeling_problem.html">
   Covid Modeling with PyMC3 - Problem Statement
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Production/Covid_modeling.html">
   Covid Modeling with PyMC3
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../_sources/docs/Intro.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sjster/statistical_computing_book/master?urlpath=tree/mini_book/docs/Intro.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/sjster/statistical_computing_book/blob/master/mini_book/docs/Intro.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#introduction">
   Introduction
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#likelihood">
     Likelihood
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#inference">
     Inference
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#the-bayesian-and-the-frequentist">
   The Bayesian and the Frequentist
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-bayesian">
     The Bayesian
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-frequentist">
     The Frequentist
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#the-differences-explained">
     The Differences Explained
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#id1">
   Inference
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#features-of-bayesian-inference">
     Features of Bayesian Inference
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#features-of-frequentist-inference">
     Features of Frequentist Inference
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#hypothesis-testing">
     Hypothesis Testing
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graded-questions-15-mins">
   GRADED QUESTIONS (15 mins)
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="bayesian-vs-frequentist-statistics">
<h1>Bayesian vs. Frequentist Statistics<a class="headerlink" href="#bayesian-vs-frequentist-statistics" title="Permalink to this headline">¶</a></h1>
<div class="section" id="introduction">
<h2>Introduction<a class="headerlink" href="#introduction" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://www.amazon.com/Students-Guide-Bayesian-Statistics/dp/1473916356">1. Ben Lambert - A Student’s Guide to Bayesian Statistics</a></p>
<p><a class="reference external" href="https://link.springer.com/chapter/10.1007%2F978-0-387-09612-4_9">2. Bayesian Evaluation of Informative Hypotheses</a></p>
<div class="section" id="likelihood">
<h3>Likelihood<a class="headerlink" href="#likelihood" title="Permalink to this headline">¶</a></h3>
<p>Likelihood is a type of probability except that this describes the probability of the data that has already been observed given a certain hypothesis parameter. Probability, in general, refers to the possibility of occurence of future events. It is worthwhile to draw the distinction between probability and likelihood here. Probability refers to varying data given a fixed hypothesis parameter(s). Likelihood refers to variable hypothesis for the fixed observed data. If ‘x’ represents our data and \(\theta\) represents our hypothesis parameter, the Likelihood is written as</p>
<div class="math notranslate nohighlight">
\[ Likelihood(\theta | data) = P(x | \theta) \]</div>
<p>It is worth pointing out likelihood does not form a valid probability since the Probability Density Function formed by varying theta with the observed data does not integrate to 1. It is for this reason that the term likelihood exists, i.e. to remind us that it is not a valid probability density.</p>
</div>
<div class="section" id="inference">
<h3>Inference<a class="headerlink" href="#inference" title="Permalink to this headline">¶</a></h3>
<p>Inference refers to the process of identifying the distribution for the parameters that represent our hypothesis. However, this is a distinctly Bayesian definition of Inference, since a Frequentist believes in the existence of true parameter. We will spend some time understanding these differences in the next section. The posterior distribution is computed during the inference step and can be denoted as</p>
<div class="math notranslate nohighlight">
\[ P(\theta | x) \]</div>
</div>
</div>
<div class="section" id="the-bayesian-and-the-frequentist">
<h2>The Bayesian and the Frequentist<a class="headerlink" href="#the-bayesian-and-the-frequentist" title="Permalink to this headline">¶</a></h2>
<p>In frequentist statistics, our probabilities are only based on what we have observed. In Bayesian statistics, we also use our prior understanding of the environment or the system that generated the observations to infer the possibility of future events. As Ben Lambert says in his book ‘A Students Guide to Bayesian Statistics’, ‘for Bayesians it is unnecessary for events to be repeatable in order to define a probability. They are merely abstractions to help express our uncertainty’. The fundamental difference between a Bayesian and a Frequentist point of view is determined by how one treats parameters and data. For a Frequentist, the uncertainty associated with a probability assigned to an event comes from randomness (aleatoric uncertainty), they do not consider the uncertainty arising from a lack of knowledge (epistemic uncertainty).</p>
<div class="section" id="the-bayesian">
<h3>The Bayesian<a class="headerlink" href="#the-bayesian" title="Permalink to this headline">¶</a></h3>
<p>Data is fixed and in light of new data we update our beliefs. The belief represented by some parameter is a random variable that we update. They are comfortable with the idea that our knowledge of these parameters evolve over time. In the case where there is a true parameter value that can be estimated, Bayes theorem allows one to specify an uncertainty over this parameter.</p>
</div>
<div class="section" id="the-frequentist">
<h3>The Frequentist<a class="headerlink" href="#the-frequentist" title="Permalink to this headline">¶</a></h3>
<p>Frequentist - The data is the result of sampling from a random process. They see the data as varying and the parameter of this random process that generates the data as being fixed. They view this parameter as being the average of an infinite number of experiments. This approach is particularly problematic for events that cannot be repeated.</p>
</div>
<div class="section" id="the-differences-explained">
<h3>The Differences Explained<a class="headerlink" href="#the-differences-explained" title="Permalink to this headline">¶</a></h3>
<p>Ben Lambert’s example of the US elections helps to illustrate this subtle difference.</p>
<p><strong>Bayesian</strong> - The probability of the Democrats winning the election is 0.75.</p>
<p><strong>Frequentist</strong> - Since there is only one sample that can be obtained for a particular election, this is not a repeatable exercise and we cannot sample from a population of outcomes for this election.</p>
<p>For example, in a coin flip that is repeated 5 times where 4 heads show up, the probability of heads for a frequentist would be 4/5. The distribution that represents the generative process of getting a heads or a tails from a coin toss can be represented by the parameter \(\theta\). When we want to determine the number of heads from ‘n’ coin tosses, this can be modeled using a Binomial distribution. If ‘O’ represents the observed outcome and ‘H’ represents the outcome being heads, then the likelihood can be written as</p>
<div class="math notranslate nohighlight">
\[Likelihood = P(O=H/ \; \theta) = 0.8\]</div>
<p>Frequentists use this as the probability of the parameter \(\theta\).</p>
<p>However, in a Bayesian approach to estimate the posterior probability of the parameter <span class="math notranslate nohighlight">\(\theta\)</span> when a heads shows up, given that the prior probability <span class="math notranslate nohighlight">\(P(\theta)\)</span> = 0.5</p>
<div class="math notranslate nohighlight">
\[ Posterior \propto Likelihood \times Prior \longrightarrow P(\theta | O = H) \propto P( O = H | \theta) \times P(\theta) \]</div>
<div class="math notranslate nohighlight">
\[ Posterior \propto 0.8 * 0.5 \]</div>
<p><strong>To summarize: In Inference, we want to measure the probability of a hypothesis given certain data. Frequentists choose a hypothesis and determine the probability of the data given this hypothesis. They then use this as evidence of the hypothesis. Bayesians go one step further to invert this to get the probability of the hypothesis given the data.</strong></p>
<p>Suppose that you are measuring the heights of citizens in your state and there is a <strong>hypothesis</strong> that their heights are normally distributed with mean  5.75ft and a standard deviation of 0.4ft. We observe a person with a height of 6.5ft. Frequentists would have to compute the probability of this observation happening as</p>
<div class="math notranslate nohighlight">
\[P(h = 6.5 | N(\mu, \sigma) )\]</div>
<p>and use a threshold to determine if our original hypothesis was valid. In Bayesian statistics, the posterior is a probability distribution of the parameters of our hypothesis given our observation of seeing a sample with height 6.5ft</p>
<div class="math notranslate nohighlight">
\[P(N(\mu, \sigma) | h = 6.5\]</div>
<p>However, in this case we use a subjective prior to compute this posterior. We trade an arbitrary threshold (usually 1% or 5% depending on the problem being solved) in frequentist statistics for a subjective prior in Bayesian statistics. One could argue that both of these are different ways to incorporate domain-knowledge and subjective expertise into the decision making process. Bayesians, however, do not need to accept or reject a hypothesis since they have the full distribution available as a result of the posterior and can therefore quantify the uncertainty with the hypothesis. More information about hypothesis testing is in the section below.</p>
</div>
</div>
<div class="section" id="id1">
<h2>Inference<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h2>
<p><a class="reference external" href="https://ocw.mit.edu/courses/mathematics/18-05-introduction-to-probability-and-statistics-spring-2014/readings/MIT18_05S14_Reading20.pdf">Reference - MIT Course Notes, Jeremy Orloff and Jonathan Bloom</a></p>
<p><a class="reference external" href="https://education.arcus.chop.edu/null-hypothesis-testing/">Reference - Null Hypothesis Significance Testing (NHST)</a></p>
<div class="section" id="features-of-bayesian-inference">
<h3>Features of Bayesian Inference<a class="headerlink" href="#features-of-bayesian-inference" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>Assign a probability to both the hypothesis (Posterior) and the data (Likelihood)</p></li>
<li><p>Utilizes expert knowledge through the formulation of ‘subjective’ priors. The use of priors has been a source of debate, however when this is clearly stated it allows everyone to understand and challenge the assumptions behind the results possibly allowing for refinement of the priors.</p></li>
<li><p>Can be computationally expensive to compute the posterior due to the need to integrate over several parameters. A lot of the times, we have to resort to approximate techniques since the integrals associated with the posterior calculation in Bayesian statistics cannot be computed analytically. A number of approximation techniques are employed.</p>
<ol class="simple">
<li><p>Laplacian approximation</p></li>
<li><p>Variational approximation</p></li>
<li><p>Monte Carlo techniques (the subject of this Specialization)</p></li>
<li><p>Message passing algorithms</p></li>
</ol>
</li>
</ol>
<p><em><strong>The primary advantage to using a Bayesian approach is that it allows us to quantify the uncertainty associated with the parameters of interest.</strong></em></p>
</div>
<div class="section" id="features-of-frequentist-inference">
<h3>Features of Frequentist Inference<a class="headerlink" href="#features-of-frequentist-inference" title="Permalink to this headline">¶</a></h3>
<ol class="simple">
<li><p>No probability for the hypothesis (no posterior)</p></li>
<li><p>Based on NHST</p></li>
<li><p>No use of a prior, but subjectivity still exists through the use of p-values</p></li>
<li><p>Less computationally intensive</p></li>
<li><p>One disadvantage of the Maximum Likelihood approach used in Frequentist methods is that the model is prone to overfitting.</p></li>
</ol>
</div>
<div class="section" id="hypothesis-testing">
<h3>Hypothesis Testing<a class="headerlink" href="#hypothesis-testing" title="Permalink to this headline">¶</a></h3>
<p>Hypothesis testing using significance testing is performed by formulating a null hypothesis, and testing the probability of an event having occurred under this null hypothesis. If this probability is less than a predefined theshold (usually 5% but is very much context dependent), we reject the null hypothesis otherwise we say that the event has a reasonable chance of occuring under the null hypothesis. The general notion behind the null hypothesis is that this is a hypothesis we want to prove wrong, the opposite of the null hypothesis is the alternate hypothesis. P-values and confidence intervals are routinely used in research to convey the credibility of experiments. However, these p-values depend on the <a class="reference external" href="https://en.wikipedia.org/wiki/Likelihood_principle#Experimental_design_arguments_on_the_likelihood_principle">experimental setup</a>.</p>
<p>Suppose we have a group of measurements for temperatures in Virginia for the month of December in 2020. It turns out that the mean temperature is 45F. The mean temperature across all the years from 1980 to 2020 was 50F. We want to test if this was an anomaly or not using NHST with p-values. Our null hypothesis here is that this is not an anomaly and the alternate hypothesis is that this was an unusually colder December. We have to select a threshold to accept or reject our hypothesis, as mentioned above this threshold is usually selected to be 5%. In order to confirm our suspicion we must reject the null hypothesis. We start by obtaining the probability of the data occurring under the null hypothesis, i.e. the probability of the mean temperature in 2020 given the distribution of mean temperatures from 1980 to 2020. If this probability is less that 5%, we reject the null hypothesis saying that it is unlikely for the data in 2020 to have come from the null hypothesis distribution. It is therefore a colder winter than Virginia is accustomed to seeing. If the probability is greater than 5%, we fail to reject the null hypothesis since there is some reason to believe (based on our subjective threshold) that this data could have been produced by the null hypothesis distribution. It is therefore no colder than expected and hence not an anomaly.</p>
</div>
</div>
<div class="section" id="graded-questions-15-mins">
<h2>GRADED QUESTIONS (15 mins)<a class="headerlink" href="#graded-questions-15-mins" title="Permalink to this headline">¶</a></h2>
<ol>
<li><p>In the Bayesian approach, it is unnecessary for events to be repeatable in order to define a probability</p>
<p>a. True</p>
<p>b. False</p>
</li>
<li><p>Bayesians use a prior to incorporate previous knowledge to make inferences while Frequentists do not</p>
<p>a. True</p>
<p>b. False</p>
</li>
<li><p>One way Frequentists incorporate their domain knowledge in through the use of p-values to reject a hypothesis</p>
<p>a. True</p>
<p>b. False</p>
</li>
<li><p>Frequentists express a probability over a hypothesis while Bayesians do not</p>
<p>a. True</p>
<p>b. False</p>
</li>
<li><p>Bayesians do not need to accept or reject a hypothesis since they have the full distribution of the hypothesis allowing them to quantify the uncertainty of that hypothesis</p>
<p>a. True</p>
<p>b. False</p>
</li>
</ol>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "sjster/statistical_computing_book",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Srijith Rajamohan, Ph.D.<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>
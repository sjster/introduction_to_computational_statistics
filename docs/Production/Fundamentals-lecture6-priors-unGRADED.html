
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" />
    <title>Priors &#8212; Introduction to Computational Statistics with PyMC3</title>
    
  <link href="../../_static/css/theme.css" rel="stylesheet" />
  <link href="../../_static/css/index.c5995385ac14fb8791e8eb36b4908be2.css" rel="stylesheet" />

    
  <link rel="stylesheet"
    href="../../_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="../../_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    
      

    
    <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="stylesheet" href="../../_static/sphinx-book-theme.css?digest=c3fdc42140077d1ad13ad2f1588a4309" type="text/css" />
    <link rel="stylesheet" type="text/css" href="../../_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-main.c949a650a448cc0ae9fd3441c0e17fb0.css" />
    <link rel="stylesheet" type="text/css" href="../../_static/panels-variables.06eb56fa6e07937060861dad626602ad.css" />
    
  <link rel="preload" as="script" href="../../_static/js/index.1c5a1a01449ed65a7b51.js">

    <script id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
    <script src="../../_static/jquery.js"></script>
    <script src="../../_static/underscore.js"></script>
    <script src="../../_static/doctools.js"></script>
    <script src="../../_static/togglebutton.js"></script>
    <script src="../../_static/clipboard.min.js"></script>
    <script src="../../_static/copybutton.js"></script>
    <script >var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="../../_static/sphinx-book-theme.12a9622fbb08dcb3a2a40b2c02b83a57.js"></script>
    <script async="async" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.7/latest.js?config=TeX-AMS-MML_HTMLorMML"></script>
    <script type="text/x-mathjax-config">MathJax.Hub.Config({"tex2jax": {"inlineMath": [["\\(", "\\)"]], "displayMath": [["\\[", "\\]"]], "processRefs": false, "processEnvironments": false}})</script>
    <script async="async" src="https://unpkg.com/thebe@0.5.1/lib/index.js"></script>
    <script >
        const thebe_selector = ".thebe,.cell"
        const thebe_selector_input = "pre,.cell_input div.highlight"
        const thebe_selector_output = ".output,.cell_output"
    </script>
    <script async="async" src="../../_static/sphinx-thebe.js"></script>
    <script src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" />
    <link rel="next" title="Bayesian vs. Frequentist Statistics" href="Intro.html" />
    <link rel="prev" title="Inference and Decisions" href="Fundamentals-lecture5b-inference-decisions-unGRADED.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="en" />
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="80">
    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<div class="col-12 col-md-3 bd-sidebar site-navigation show" id="site-navigation">
    
        <div class="navbar-brand-box">
    <a class="navbar-brand text-wrap" href="../../index.html">
      
        <!-- `logo` is deprecated in Sphinx 4.0, so remove this when we stop supporting 3 -->
        
      
      
      <img src="../../_static/logo_large.png" class="logo" alt="logo">
      
      
      <h1 class="site-logo" id="site-title">Introduction to Computational Statistics with PyMC3</h1>
      
    </a>
</div><form class="bd-search d-flex align-items-center" action="../../search.html" method="get">
  <i class="icon fas fa-search"></i>
  <input type="search" class="form-control" name="q" id="search-input" placeholder="Search this book..." aria-label="Search this book..." autocomplete="off" >
</form><nav class="bd-links" id="bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item active">
        <p class="caption">
 <span class="caption-text">
  Getting started
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="About.html">
   The What, Why and Whom…
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="getting_started.html">
   Setting up Your Python Environment
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Databricks.html">
   Introduction to the Databricks Environment
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction to Bayesian Statistics
 </span>
</p>
<ul class="current nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="Fundamentals-lecture1-belief-and-probability-unGRADED.html">
   Probability
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Fundamentals-lecture2-manipulating-probability-unGRADED.html">
   Probability - II
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Fundamentals-lecture3-intro-distributions-unGRADED.html">
   Distributions, central tendency, and shape parameters
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Fundamentals-lecture4-MoM-MLE-unGRADED.html">
   Parameter Estimation
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Fundamentals-lecture5a-Basics-of-Bayes-unGRADED.html">
   Introduction to the Bayes Theorem
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Fundamentals-lecture5b-inference-decisions-unGRADED.html">
   Inference and Decisions
  </a>
 </li>
 <li class="toctree-l1 current active">
  <a class="current reference internal" href="#">
   Priors
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Intro.html">
   Bayesian vs. Frequentist Statistics
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Distributions.html">
   Introduction to Common Distributions
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Sampling.html">
   Sampling Algorithms
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  Introduction to Monte Carlo Methods
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="BayesianInference.html">
   Topics in Model Performance
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="MonteCarlo.html">
   Introduction to Monte Carlo Methods
  </a>
 </li>
</ul>
<p class="caption">
 <span class="caption-text">
  PyMC3 for Bayesian Modeling and Inference
 </span>
</p>
<ul class="nav bd-sidenav">
 <li class="toctree-l1">
  <a class="reference internal" href="PyMC3.html">
   Introduction to PyMC3
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Reparameterization.html">
   Centered vs. Non-centered Parameterization
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Covid_modeling_problem.html">
   Covid Modeling with PyMC3 - Problem Statement
  </a>
 </li>
 <li class="toctree-l1">
  <a class="reference internal" href="Covid_modeling.html">
   Covid Modeling with PyMC3
  </a>
 </li>
</ul>

    </div>
</nav> <!-- To handle the deprecated key -->

<div class="navbar_extra_footer">
  Powered by <a href="https://jupyterbook.org">Jupyter Book</a>
</div>

</div>


          


          
<main class="col py-md-3 pl-md-4 bd-content overflow-auto" role="main">
    
    <div class="topbar container-xl fixed-top">
    <div class="topbar-contents row">
        <div class="col-12 col-md-3 bd-topbar-whitespace site-navigation show"></div>
        <div class="col pl-md-4 topbar-main">
            
            <button id="navbar-toggler" class="navbar-toggler ml-0" type="button" data-toggle="collapse"
                data-toggle="tooltip" data-placement="bottom" data-target=".site-navigation" aria-controls="navbar-menu"
                aria-expanded="true" aria-label="Toggle navigation" aria-controls="site-navigation"
                title="Toggle navigation" data-toggle="tooltip" data-placement="left">
                <i class="fas fa-bars"></i>
                <i class="fas fa-arrow-left"></i>
                <i class="fas fa-arrow-up"></i>
            </button>
            
            
<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn" aria-label="Download this page"><i
            class="fas fa-download"></i></button>

    <div class="dropdown-buttons">
        <!-- ipynb file if we had a myst markdown file -->
        
        <!-- Download raw file -->
        <a class="dropdown-buttons" href="../../_sources/docs/Production/Fundamentals-lecture6-priors-unGRADED.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Download source file" data-toggle="tooltip"
                data-placement="left">.ipynb</button></a>
        <!-- Download PDF via print -->
        <button type="button" id="download-print" class="btn btn-secondary topbarbtn" title="Print to PDF"
            onClick="window.print()" data-toggle="tooltip" data-placement="left">.pdf</button>
    </div>
</div>

            <!-- Source interaction buttons -->

            <!-- Full screen (wrap in <a> to have style consistency -->

<a class="full-screen-button"><button type="button" class="btn btn-secondary topbarbtn" data-toggle="tooltip"
        data-placement="bottom" onclick="toggleFullScreen()" aria-label="Fullscreen mode"
        title="Fullscreen mode"><i
            class="fas fa-expand"></i></button></a>

            <!-- Launch buttons -->

<div class="dropdown-buttons-trigger">
    <button id="dropdown-buttons-trigger" class="btn btn-secondary topbarbtn"
        aria-label="Launch interactive content"><i class="fas fa-rocket"></i></button>
    <div class="dropdown-buttons">
        
        <a class="binder-button" href="https://mybinder.org/v2/gh/sjster/statistical_computing_book/master?urlpath=tree/mini_book/docs/Production/Fundamentals-lecture6-priors-unGRADED.ipynb"><button type="button"
                class="btn btn-secondary topbarbtn" title="Launch Binder" data-toggle="tooltip"
                data-placement="left"><img class="binder-button-logo"
                    src="../../_static/images/logo_binder.svg"
                    alt="Interact on binder">Binder</button></a>
        
        
        
        <a class="colab-button" href="https://colab.research.google.com/github/sjster/statistical_computing_book/blob/master/mini_book/docs/Production/Fundamentals-lecture6-priors-unGRADED.ipynb"><button type="button" class="btn btn-secondary topbarbtn"
                title="Launch Colab" data-toggle="tooltip" data-placement="left"><img class="colab-button-logo"
                    src="../../_static/images/logo_colab.png"
                    alt="Interact on Colab">Colab</button></a>
        
        <button type="button" class="btn btn-secondary topbarbtn"
            onclick="initThebeSBT()" title="Launch Thebe" data-toggle="tooltip" data-placement="left"><i
                class="fas fa-play"></i><span style="margin-left: .4em;">Live Code</span></button>
        
    </div>
</div>

        </div>

        <!-- Table of contents -->
        <div class="d-none d-md-block col-md-2 bd-toc show">
            
            <div class="tocsection onthispage pt-5 pb-3">
                <i class="fas fa-list"></i> Contents
            </div>
            <nav id="bd-toc-nav" aria-label="Page">
                <ul class="visible nav section-nav flex-column">
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#prior-likelihood">
   Prior * Likelihood
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#conjugate-priors">
     Conjugate priors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#noninformative-priors">
     Noninformative priors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#jeffrey-s-non-informative-prior">
     Jeffrey’s non-informative prior
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#informative-priors">
     Informative priors
    </a>
   </li>
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#weakly-informative-or-vague-priors">
     Weakly informative or vague priors
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#sensitivity-analysis-of-priors">
   Sensitivity analysis of priors
  </a>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#likelihood-prior">
   Likelihood * prior
  </a>
  <ul class="nav section-nav flex-column">
   <li class="toc-h3 nav-item toc-entry">
    <a class="reference internal nav-link" href="#example-binomial-beta">
     Example binomial-beta
    </a>
   </li>
  </ul>
 </li>
 <li class="toc-h2 nav-item toc-entry">
  <a class="reference internal nav-link" href="#graded-evaluation-30-mins">
   GRADED EVALUATION (30 mins)
  </a>
 </li>
</ul>

            </nav>
        </div>
    </div>
</div>
    <div id="main-content" class="row">
        <div class="col-12 col-md-9 pl-md-3 pr-md-0">
        
              <div>
                
  <div class="section" id="priors">
<h1>Priors<a class="headerlink" href="#priors" title="Permalink to this headline">¶</a></h1>
<p>Bayes’ Rule gives us a method to update our beleifs based on prior knowledge.  In words, this looks like:</p>
<div class="math notranslate nohighlight">
\[posterior = \frac{likelihood \ast prior}{marginal}\]</div>
<p>Which can also be stated as:</p>
<div class="math notranslate nohighlight">
\[posterior = \frac{likelihood \ast prior}{evidence}\]</div>
<p>Or, being explicit about data \(\mathcal{D}\) and parameters \(\theta\), we might write it as:</p>
<div class="math notranslate nohighlight">
\[p(\theta|\mathcal{D}) = \frac{p(\mathcal{D}|\theta) \ast p(\theta)}{p(\mathcal{D})}\]</div>
<p>This is a change in frame of reference with respect to variable data and fixed parameters which is the hallmark of Bayesian thinking.  After you observe the data, it is fixed.  This session we will talk about priors, how to think about the prior and what influence it has on the posterior.</p>
<p>General references:</p>
<ul class="simple">
<li><p>Pattern Recognition and Machine Learning (9780387310732) Bishop, Christopher M.</p></li>
<li><p>Statistical Inference (9780534243128): Casella, George, Berger, Roger L.</p></li>
<li><p>Probability Theory and Statistical Inference: Empirical Modeling with Observational Data (9781107185142): Spanos, A.</p></li>
<li><p>Bayesian Models: A Statistical Primer for Ecologists (9780691159287): Hobbs, N. Thompson, Hooten, Mevin B.</p></li>
<li><p>A First Course in Bayesian Statistical Methods (0387922997): Hoff, Peter D.</p></li>
</ul>
<br>
<br>
<hr style="border:2px solid blue"> </hr><div class="section" id="prior-likelihood">
<h2>Prior * Likelihood<a class="headerlink" href="#prior-likelihood" title="Permalink to this headline">¶</a></h2>
<p>In learning about Bayes’ Rule, we saw how our prior information about the occurrence of an event is updated through the conditional probability of an event.  We are now considering the case where both the conditional probability of data|parameters and the prior are distributions (including possibly the uniform).</p>
<p>As related to Bayes’ Rule, the prior is the unconditional probability of the parameters before (new) data.  Priors can come from a number of sources including</p>
<ul class="simple">
<li><p>past experiments or experience</p></li>
<li><p>some sort of desire for balance or weighting in a decision</p></li>
<li><p>non-informative, but objective</p></li>
<li><p>mathematical convenience</p></li>
</ul>
<p>Choice of the prior is as much about what is currently known about the parameters as it is about the goal of the analysis.  This makes the choice of prior subjective and often contested.  How do we choose priors?  Two broad categories could include:</p>
<ul class="simple">
<li><p>noninformative</p></li>
<li><p>informative</p></li>
</ul>
<p>The priors can also be <strong>proper</strong>, ie conform to the rules of probability and integrate to 1, or <strong>improper</strong>.  As an example, consider the uniform distribution as a prior to the mean in a normal distribution, we want to apply equal weights to all possible values of \(\mu\).  However, \(\mu \in [-\infty,\infty]\) and our uniform prior must span the same range. Jeffrey’s prior is another example of an (often) improper prior discussed below.</p>
<p>The form of the prior is also a choice.  Convenient choices of priors can lead to closed form solutions for the posterior.</p>
<br>
<br>
<hr style="border:2px solid blue"> </hr>
<div class="section" id="conjugate-priors">
<h3>Conjugate priors<a class="headerlink" href="#conjugate-priors" title="Permalink to this headline">¶</a></h3>
<p>Conjugate priors are priors that induce a known distribution in the posterior.  When computing the posterior probability, if we have a justifiable reason for using pairing the likelihood with a conjugate prior, we will find the posterior probability is a known distribution.  For example, consider</p>
<div class="math notranslate nohighlight">
\[X \sim Bern(\theta)\]</div>
<p>The likelihood takes the form:</p>
<p>\(f(x|\theta) = \prod_{i=1}^n \theta^{x_i}(1-\theta)^{1-x_i} = \theta^{k}(1-\theta)^{n-k}\), where \(k=\sum x_i\),shifting our focus from x, to \(\theta\), if \(\theta\) were the random variable, it appears to have the functional kernel of the distribution of a beta.  If we assume the prior take the form of a beta distribution (dropping all the constants):</p>
<div class="math notranslate nohighlight">
\[f(x|\theta) \ast p(\theta) \propto \theta^{k}(1-\theta)^{n-k} \ast \theta^{\alpha-1}(1-\theta)^{\beta-1} = \theta^{\alpha+k-1}(1-\theta)^{\beta+n-k-1}\]</div>
<p>Which we recognize as \(Beta(\alpha+k,\beta+n-k)\) in \(\theta\).</p>
<p>IF, we are modelling a Bernoulli process and use a prior with a Beta distribution, our posterior has a known distribution with parameters as shown.</p>
<p>Common conjugate priors by likelihood type:<br />
likelihood - prior<br />
Binomial - beta<br />
Poisson - Gamma<br />
Normal (known \(\sigma^2\)) - Normal<br />
Normal (known \(\mu\)) - inverse-gamma<br />
Multivariate normal (known \(\Sigma\)) - inverse-Wishart</p>
<br>
<br>
<hr style="border:2px solid blue"> </hr>
</div>
<div class="section" id="noninformative-priors">
<h3>Noninformative priors<a class="headerlink" href="#noninformative-priors" title="Permalink to this headline">¶</a></h3>
<p>Noninformative priors are priors that suggest ignorance as to the parameters.  These are sometimes called vague or diffuse priors.  The priors generally cover the region of the parameter space relatively smoothly.  Common noninformative priors include <span class="math notranslate nohighlight">\(unif(-1000,1000)\)</span>, <span class="math notranslate nohighlight">\(N(0,10,000)\)</span>.  Note that seeminly vague priors can actually be strongly informative.  Consider the case of modeling a binary model for y following a Bernoulli distribution.  A common modeling technique would be to transform the problem using the logit function.  For instance:</p>
<div class="math notranslate nohighlight">
\[y \sim Bern(p)\]</div>
<div class="math notranslate nohighlight">
\[p=logit^{-1}(\beta_0 + \beta_1)\]</div>
<p>Placing priors \([\beta_0,\beta_1] \sim N(0,100)\) places undue weight on 0 through the transform while using a weakly informative prior \([\beta_0,\beta_1] \sim N(0,2^2)\) gives a more diffuse effect on the parameter posterior.</p>
<br>
<br>
<hr style="border:2px solid blue"> </hr>
</div>
<div class="section" id="jeffrey-s-non-informative-prior">
<h3>Jeffrey’s non-informative prior<a class="headerlink" href="#jeffrey-s-non-informative-prior" title="Permalink to this headline">¶</a></h3>
<p>The Jeffrey’s prior is a non-informative prior that is derived from the Fisher information.  This prior is non-informative in that we do not specify prior information, but it is informative in that we use the data to information to shape the prior.  Specifically, Fisher’s information tells us how much information about <span class="math notranslate nohighlight">\(\theta\)</span> is included in the data.  Formally, the Jeffrey’s prior is derived by:</p>
<p>\(p(\theta) \propto \sqrt{I_n(\theta)}\), where</p>
<div class="math notranslate nohighlight">
\[I_n(\theta) = E_{\theta}\big\{\big(\frac{\partial ln f(\theta)}{\partial \theta}\big)\big\} = -E_{\theta}\big\{\big(\frac{\partial^2 ln L(\theta)}{\partial \theta^2}\big)\big\}\]</div>
<p>This is a <span class="math notranslate nohighlight">\(pxp\)</span> matrix of partial derivatives when there are p parameters.</p>
<p>Working through an example, let</p>
<p>\(X \sim gamma(\alpha,\beta)\), assuming \(\alpha\) is known and \(\beta\) is unknown.  Through the likelihood, one can work out the Fisher’s information to be</p>
<p>\(I_n(\beta) = \frac{n\alpha}{\beta^2}\) leading to the Jeffrey’s prior for \(\beta\):</p>
<div class="math notranslate nohighlight">
\[p(\beta) \propto \sqrt{\frac{n\alpha}{\beta^2}} \propto 1\]</div>
<p>Note that Jeffrey’s priors are not guaranteed to be proper.  Perhaps most importantly, Jeffrey’s priors are stable under reparameterization.</p>
<br>
<br>
<hr style="border:2px solid blue"> </hr>
</div>
<div class="section" id="informative-priors">
<h3>Informative priors<a class="headerlink" href="#informative-priors" title="Permalink to this headline">¶</a></h3>
<p>Informative priors are explicitly chosen to represent current knowledge or belief about the parameters of interest.  When choosing informative priors, one can also choose the form of the prior.  As an example, suppose we are back at tossing coins.  We were given a new coin from a friend and were told it would generate heads with P(heads) = 0.75.  We conduct a new experiment to characterize the distribution of <span class="math notranslate nohighlight">\(\theta\)</span>.  We will see that a computationally convenient distribution on the prior is a beta(a,b) when dealing with Bernoulli trials.  What do we choose for the parameters of the beta distribution?  To incorporate the information we have, we might choose a beta having a mean close to 0.75 to represent the information given to us.  We also have the ability to choose the precision/scale.  We can tune that to represent some amount of disbelief in the unfairness of the coin.  Looking at the plots below, we could use the beta(6.9,3) or beta(16,6) distribution as examples.  These were chosen empirically through plotting and calculating the mean and modes.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">from</span> <span class="nn">scipy.stats</span> <span class="kn">import</span> <span class="n">beta</span>
<span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">import</span> <span class="nn">matplotlib.pyplot</span> <span class="k">as</span> <span class="nn">plt</span>

<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>

<span class="n">y1</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">,</span> <span class="mf">0.5</span><span class="p">)</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">y3</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">y4</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">6.9</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">y5</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="s2">&quot;-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;1/2,1/2&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;1,1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y3</span><span class="p">,</span> <span class="s2">&quot;g--&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;3,3&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y4</span><span class="p">,</span> <span class="s2">&quot;b--&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;6.9,3&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y5</span><span class="p">,</span> <span class="s2">&quot;y--&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;16,6&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Fundamentals-lecture6-priors-unGRADED_2_0.png" src="../../_images/Fundamentals-lecture6-priors-unGRADED_2_0.png" />
</div>
</div>
</div>
<div class="section" id="weakly-informative-or-vague-priors">
<h3>Weakly informative or vague priors<a class="headerlink" href="#weakly-informative-or-vague-priors" title="Permalink to this headline">¶</a></h3>
<p>In the plots above, we see that we can tune our prior belief using the mean or even mode to center our beleif and variance as a measure of our strength of belief.  For the beta,(6.9,3) this may look like:</p>
<div class="math notranslate nohighlight">
\[E[x] = \frac{a}{a+b} = 0.70\]</div>
<div class="math notranslate nohighlight">
\[mode(x) = \frac{a-1}{(a+b-2)} = 0.77\]</div>
<div class="math notranslate nohighlight">
\[Var(x) = \frac{ab}{(a+b)^2(a+b+1)} = 0.05\]</div>
<p>Tuning the prior to include slightly more confidence in the prior information may suggest a beta(16,6) which reduces the variance to 0.028.  These priors are vague in that the mass of the prior is still diffuse allowing the data to drive the posterior through the likelihood.</p>
<br>
<br>
<hr style="border:2px solid blue"> </hr>
</div>
</div>
<div class="section" id="sensitivity-analysis-of-priors">
<h2>Sensitivity analysis of priors<a class="headerlink" href="#sensitivity-analysis-of-priors" title="Permalink to this headline">¶</a></h2>
<p>The general approach to using priors in models is to start with some justification for a prior, run the analysis, then come up with competing priors and reexamine the conclusions under the alternative priors.  The results of the final model and the analysis of the sensitivity of the analysis to the choice of prior are written up as a package.</p>
<p>For a discussion of steps and methods to use in a sensitivity analysis, see:
Bayesian Data Analysis by Andrew Gelman, John Carlin, Hal Stern, David Dunson, Aki Vehtari, and Donald Rubin.</p>
<br>
<br>
<hr style="border:2px solid blue"> </hr>
</div>
<div class="section" id="likelihood-prior">
<h2>Likelihood * prior<a class="headerlink" href="#likelihood-prior" title="Permalink to this headline">¶</a></h2>
<p>What we have been talking about is priors and thier influence on the likelihood in development of the posterior.  A few examples should help.</p>
<br>
<br>
<hr style="border:2px solid blue"> </hr>
<div class="section" id="example-binomial-beta">
<h3>Example binomial-beta<a class="headerlink" href="#example-binomial-beta" title="Permalink to this headline">¶</a></h3>
<p>Let us first consider the coin toss experiment above, as a reminder, we are given a presumably unfair coin with p(heads)=0.75.  Let us collect new data and analyze the posterior.  First, we are going to collect a series of tosses, so the likelihood is best given by a binomial distribution.  Examining the likelihood from the perspective of the parameter, \(\theta\), we note both that the functional form of the likelihood appears to be that of a beta and the support of a beta, [0,1], vs the valid values of \(\theta\) match.</p>
<div class="math notranslate nohighlight">
\[p(\theta|x) \propto f(x|\theta) \ast p(\theta) \propto \prod_i \big [ \theta^{x_i} (1-\theta)^{n-x_i} \big ] \ast \big [\frac{\Gamma(\alpha+\beta)}{\Gamma(\alpha)\Gamma(\beta)}\theta^{\alpha-1}(1-\theta)^{\beta-1} \big ]\]</div>
<p>Simplifying a bit, we get:</p>
<div class="math notranslate nohighlight">
\[p(\theta|x) \propto \theta^{\alpha+n\bar{x}-1} (1-\theta)^{\beta+n-n\bar{x}-1}\]</div>
<p>Following along the discussion above, we note that the things we have that are tunable (hyperparameters), are \(\alpha\) and \(\beta\).  Here, we are using the value of \(\theta\), but to what extent do we believe the friend who gave us the coin and the tip?  Let us simulate three scenarios: a) we don’t believe the friend at all, b) we give some credience to the friend and c) we are strong in our trust.  We will represent these scenarios using beta(1,1), beta(6.9,3),beta(16,6).  Note that we know what the final distribution is, it is a beta(\(\alpha+n\bar{x}\), \(\beta+n-n\bar{x}\)).  We could simply compute the mean, variance etc.  Instead, let’s visualize the outcomes.  Assume first, we tossed 10 coins and observed 7 heads.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### assume the coin is as advertized, could have drew from rbinom(p=0.75)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">7</span> <span class="c1">#used x</span>

<span class="n">p1</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">p2</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">6.9</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">p3</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="o">+</span><span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="p">)</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mf">6.9</span><span class="p">,</span> <span class="mi">3</span><span class="o">+</span><span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="p">)</span>
<span class="n">y3</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="o">+</span><span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p1</span><span class="p">,</span> <span class="s2">&quot;b-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;prior = 1,1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p2</span><span class="p">,</span> <span class="s2">&quot;r-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;prior = 6.9,3&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p3</span><span class="p">,</span> <span class="s2">&quot;g-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;prior = 16,6&quot;</span><span class="p">)</span>

<span class="c1">## posteriors are labeled with prior parameters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="s2">&quot;b--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;posterior = 1,1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;posterior = 6.9,3&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y3</span><span class="p">,</span> <span class="s2">&quot;g--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;posterior = 16,6&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">5.5</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1">## note the distributions all pull towards the likelihood, irrespective of how informative the prior was</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Fundamentals-lecture6-priors-unGRADED_4_0.png" src="../../_images/Fundamentals-lecture6-priors-unGRADED_4_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### assume the coin is perfectly fair, could have drew from rbinom(p=0.5)</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">n</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">k</span> <span class="o">=</span> <span class="mi">5</span> <span class="c1">#used x</span>

<span class="n">p1</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">p2</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">6.9</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">p3</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="o">+</span><span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="p">)</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mf">6.9</span><span class="p">,</span> <span class="mi">3</span><span class="o">+</span><span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="p">)</span>
<span class="n">y3</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k</span><span class="o">+</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="o">+</span><span class="n">n</span><span class="o">-</span><span class="n">k</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p1</span><span class="p">,</span> <span class="s2">&quot;b-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;prior = 1,1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p2</span><span class="p">,</span> <span class="s2">&quot;r-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;prior = 6.9,3&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p3</span><span class="p">,</span> <span class="s2">&quot;g-&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;prior = 16,6&quot;</span><span class="p">)</span>

<span class="c1">## posteriors are labeled with prior parameters</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="s2">&quot;b--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;posterior = 1,1&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;posterior = 6.9,3&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y3</span><span class="p">,</span> <span class="s2">&quot;g--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;posterior = 16,6&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">5.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>

<span class="c1">## again, the likelihood pulls the posterior distribution towards the theta supported by the data</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Fundamentals-lecture6-priors-unGRADED_5_0.png" src="../../_images/Fundamentals-lecture6-priors-unGRADED_5_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">### assume the coin is perfectly fair, could have drawn from rbinom(p=0.5)</span>
<span class="c1">### BUT, we trusted our friend, what does the size of the experiment do to our posterior</span>
<span class="n">x</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">linspace</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">100</span><span class="p">)</span>
<span class="n">n1</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">n2</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">n3</span> <span class="o">=</span> <span class="mi">50</span>
<span class="n">k1</span> <span class="o">=</span> <span class="mi">5</span> 
<span class="n">k2</span> <span class="o">=</span> <span class="mi">10</span>
<span class="n">k3</span> <span class="o">=</span> <span class="mi">25</span>

<span class="n">p1</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
<span class="n">p2</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mf">6.9</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span>
<span class="n">p3</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="p">)</span>
<span class="n">y1</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k1</span><span class="o">+</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="o">+</span><span class="n">n1</span><span class="o">-</span><span class="n">k1</span><span class="p">)</span>
<span class="n">y2</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k2</span><span class="o">+</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="o">+</span><span class="n">n2</span><span class="o">-</span><span class="n">k2</span><span class="p">)</span>
<span class="n">y3</span> <span class="o">=</span> <span class="n">beta</span><span class="o">.</span><span class="n">pdf</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">k3</span><span class="o">+</span><span class="mi">16</span><span class="p">,</span> <span class="mi">6</span><span class="o">+</span><span class="n">n3</span><span class="o">-</span><span class="n">k3</span><span class="p">)</span>
<span class="c1">#plt.plot(x, p1, &quot;b-&quot;, label=&quot;prior = 1,1&quot;)</span>
<span class="c1">#plt.plot(x, p2, &quot;r-&quot;,label=&quot;prior = 6.9,3&quot;)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">p3</span><span class="p">,</span> <span class="s2">&quot;y-&quot;</span><span class="p">,</span><span class="n">label</span><span class="o">=</span><span class="s2">&quot;prior = 16,6&quot;</span><span class="p">)</span>

<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y1</span><span class="p">,</span> <span class="s2">&quot;b--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;n=10&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y2</span><span class="p">,</span> <span class="s2">&quot;r--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;n=20&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">plot</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">y3</span><span class="p">,</span> <span class="s2">&quot;g--&quot;</span><span class="p">,</span> <span class="n">label</span><span class="o">=</span><span class="s2">&quot;n=50&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">legend</span><span class="p">(</span><span class="n">loc</span><span class="o">=</span><span class="s2">&quot;upper left&quot;</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">ylim</span><span class="p">(</span><span class="o">-</span><span class="mf">0.1</span><span class="p">,</span> <span class="mf">8.0</span><span class="p">)</span>
<span class="n">plt</span><span class="o">.</span><span class="n">show</span><span class="p">()</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="../../_images/Fundamentals-lecture6-priors-unGRADED_6_0.png" src="../../_images/Fundamentals-lecture6-priors-unGRADED_6_0.png" />
</div>
</div>
</div>
</div>
<div class="section" id="graded-evaluation-30-mins">
<h2>GRADED EVALUATION (30 mins)<a class="headerlink" href="#graded-evaluation-30-mins" title="Permalink to this headline">¶</a></h2>
<ol class="simple">
<li><p>For a variable \(X \sim N(\mu,\sigma^2)\) where \(\sigma^2\) is fixed and known, find the Jeffrey’s prior. Is it proper?</p></li>
</ol>
<p>a. \(p(\mu) \propto \sqrt{\frac{1}{\sigma^2}} \propto 1\) ; proper</p>
<p>b. \(p(\mu) \propto \sqrt{\frac{1}{\sigma^2}} \propto 1\) ; improper</p>
<p>c. \(p(\mu) \propto \sqrt{\frac{1}{\sigma^2}} \propto \frac{1}{\sigma^2}\) ; proper</p>
<div class="math notranslate nohighlight">
\[\]</div>
<ol class="simple">
<li><p>We are studing a Bernoulli process for which we have no prior information.  We decide to use a non-informative prior such as the beta(1,1).  Because the prior is flat [0,1], this prior will have no effect on the posterior.</p></li>
</ol>
<p>a. true</p>
<p>b. false</p>
<div class="math notranslate nohighlight">
\[\]</div>
<ol class="simple">
<li><p>The data we are modelling comes from a geometric distribution.  A good choice of prior family is:</p></li>
</ol>
<p>a. exponential; because there is only one parameter</p>
<p>b. beta; because the for of the exponential matches the kernel of the beta</p>
<p>c. uniform; due to the parameter of the exponential being a proportion</p>
<div class="math notranslate nohighlight">
\[\]</div>
<ol class="simple">
<li><p>Given a prior having mean 10 and data having mean 5, we should expect the posterior mean to lie</p></li>
</ol>
<p>a. to the left of 5</p>
<p>b. to the right of 10</p>
<p>c. between 5 and 10</p>
</div>
</div>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "sjster/statistical_computing_book",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./docs/Production"
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
        
            



<div class='prev-next-bottom'>
    
    <div id="prev">
        <a class="left-prev" href="Fundamentals-lecture5b-inference-decisions-unGRADED.html" title="previous page">
            <i class="prevnext-label fas fa-angle-left"></i>
            <div class="prevnext-info">
                <p class="prevnext-label">previous</p>
                <p class="prevnext-title">Inference and Decisions</p>
            </div>
        </a>
    </div>
     <div id="next">
        <a class="right-next" href="Intro.html" title="next page">
            <div class="prevnext-info">
                <p class="prevnext-label">next</p>
                <p class="prevnext-title">Bayesian vs. Frequentist Statistics</p>
            </div>
            <i class="prevnext-label fas fa-angle-right"></i>
        </a>
     </div>

</div>
        
        </div>
    </div>
    <footer class="footer">
    <div class="container">
      <p>
        
          By Srijith Rajamohan, Ph.D.<br/>
        
            &copy; Copyright 2021.<br/>
      </p>
    </div>
  </footer>
</main>


      </div>
    </div>
  
  <script src="../../_static/js/index.1c5a1a01449ed65a7b51.js"></script>

  
  </body>
</html>